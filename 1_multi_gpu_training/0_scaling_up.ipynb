{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c15fb0b3-888c-4713-b358-2c8961b177b1",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 alxyok\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40589b09-ef34-4bd9-8540-1beadc29aa6d",
   "metadata": {},
   "source": [
    "****************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd6e24d-405a-46fa-aa5b-1956cad4e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the root of this bootcamp\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "sys.path.append(osp.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# PyTorch Lightning abstraction basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22373b90-f912-4138-92d8-ac21ff9feb99",
   "metadata": {},
   "source": [
    "Putting it all together with PL abstraction mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edafc4-1c3c-42ae-94af-5efac1f873df",
   "metadata": {},
   "source": [
    "Let's first load all the necessary params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392a0d65-b9d8-4ad7-9b87-ef5c0973ef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import config\n",
    "\n",
    "# Ensures this Notebook's reproducibility\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "step = config.config['timestep']\n",
    "params = config.params[str(step)]['flattened']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e9166-f8ab-44a7-aca1-53b38ff0089b",
   "metadata": {},
   "source": [
    "## Model and training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da267a70-50c3-4882-9312-24ed12fcda7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MIT License\n",
      "# \n",
      "# Copyright (c) 2022 alxyok\n",
      "# \n",
      "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "# of this software and associated documentation files (the \"Software\"), to deal\n",
      "# in the Software without restriction, including without limitation the rights\n",
      "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "# copies of the Software, and to permit persons to whom the Software is\n",
      "# furnished to do so, subject to the following conditions:\n",
      "# \n",
      "# The above copyright notice and this permission notice shall be included in all\n",
      "# copies or substantial portions of the Software.\n",
      "# \n",
      "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "# SOFTWARE.\n",
      "\n",
      "import os.path as osp\n",
      "import pytorch_lightning as pl\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch_optimizer as optim\n",
      "import torchmetrics.functional as F\n",
      "from typing import List, Tuple, Union\n",
      "import sys\n",
      "\n",
      "sys.path.append(osp.abspath('..'))\n",
      "\n",
      "import config\n",
      "\n",
      "class ThreeDCorrectionModule(pl.LightningModule):\n",
      "    \n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        \n",
      "        # 'The LightningModule knows what device it is on. You can access the reference via self.device. Sometimes it is necessary to store tensors as module attributes. However, if they are not parameters they will remain on the CPU even if the module gets moved to a new device. To prevent that and remain device agnostic, register the tensor as a buffer in your modulesâ€™s __init__ method with register_buffer().'\n",
      "        self.register_buffer(\"epsilon\", torch.tensor(1.e-8))\n",
      "\n",
      "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "        return self.net(x)\n",
      "    \n",
      "    def _common_step(self, \n",
      "                     batch: List[torch.Tensor], \n",
      "                     batch_idx: int, \n",
      "                     stage: Union['train', 'test', 'val']) -> Tuple[torch.Tensor]:\n",
      "        \n",
      "        x, y = batch\n",
      "        y_hat = self(x)\n",
      "        \n",
      "        # Note: by using a metric from TorchMetrics, you don't have to manually set the sync_dist option, 'ensures that each GPU worker has the same behaviour when tracking model checkpoints, which is important for later downstream tasks such as testing the best checkpoint across all workers'.\n",
      "        loss = F.mean_squared_error(y_hat, y)\n",
      "        self.log(\n",
      "            f\"{stage}_loss\", \n",
      "            loss, prog_bar=True, \n",
      "            on_step=True, \n",
      "            batch_size=len(batch),\n",
      "            # Uncomment, for the 'test' and 'val' stages if you're not using TorchMetrics\n",
      "            # sync_dist = True\n",
      "        )\n",
      "        \n",
      "        return y_hat, loss\n",
      "    \n",
      "    def training_step(self, \n",
      "                      batch: List[torch.Tensor], \n",
      "                      batch_idx: int) -> torch.Tensor:\n",
      "        \n",
      "        _, loss = self._common_step(batch, batch_idx, \"train\")\n",
      "        \n",
      "        return loss\n",
      "\n",
      "    def validation_step(self, \n",
      "                        batch: List[torch.Tensor], \n",
      "                        batch_idx: int) -> Tuple[torch.Tensor]:\n",
      "        \n",
      "        y_hat, _ = self._common_step(batch, batch_idx, \"val\")\n",
      "\n",
      "    def test_step(self, \n",
      "                  batch: List[torch.Tensor], \n",
      "                  batch_idx: int) -> Tuple[torch.Tensor]:\n",
      "        y_hat, _ = self._common_step(batch, batch_idx, \"test\")\n",
      "    \n",
      "    def configure_optimizers(self) -> optim.Optimizer:\n",
      "        \n",
      "        return optim.AdamP(self.parameters(), lr=self.lr)\n",
      "    \n",
      "    \n",
      "    \n",
      "class LitMLP(ThreeDCorrectionModule):\n",
      "\n",
      "    \n",
      "    class Normalize(nn.Module):\n",
      "\n",
      "        def __init__(self, epsilon: torch.Tensor) -> None:\n",
      "\n",
      "            super().__init__()\n",
      "\n",
      "            self.epsilon = epsilon\n",
      "\n",
      "            step = config.config['timestep']\n",
      "            stats = torch.load(osp.join(config.data_path, f\"stats-flattened-{step}.pt\"))\n",
      "\n",
      "            self.x_mean = stats[\"x_mean\"]\n",
      "            self.x_std = stats[\"x_std\"]\n",
      "\n",
      "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "\n",
      "            # Adjusting the tensors to scale to other devices. 'When you need to create a new tensor, use type_as. This will make your code scale to any arbitrary number of GPUs or TPUs with Lightning.'\n",
      "            mean = self.x_mean.type_as(x)\n",
      "            std = self.x_std.type_as(x)\n",
      "\n",
      "            return (x - mean) / (std + self.epsilon)\n",
      "    \n",
      "    \n",
      "    def __init__(self, \n",
      "                 in_channels: int, \n",
      "                 hidden_channels: int, \n",
      "                 out_channels: int, \n",
      "                 lr: float = 1e-4) -> None:\n",
      "        \n",
      "        super().__init__()\n",
      "        \n",
      "        self.lr = lr\n",
      "        self.normalization_layer = LitMLP.Normalize(self.epsilon)\n",
      "        \n",
      "        self.net = nn.Sequential(\n",
      "            self.normalization_layer,\n",
      "            nn.Linear(in_channels, hidden_channels),\n",
      "            nn.SiLU(),\n",
      "            nn.Linear(hidden_channels, hidden_channels),\n",
      "            nn.SiLU(),\n",
      "            nn.Linear(hidden_channels, hidden_channels),\n",
      "            nn.SiLU(),\n",
      "            nn.Linear(hidden_channels, out_channels),\n",
      "        )"
     ]
    }
   ],
   "source": [
    "!cat models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a82a516-48d1-471b-9b08-758be80e258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "becf22e3-b1ab-4940-ba48-f54b3c8c0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feats = params['x_shape'][-1]\n",
    "y_feats = params['y_shape'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0fe802-3725-4985-b723-0b0c8ce68245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x number of features: 4128\n",
      "y number of features: 552\n"
     ]
    }
   ],
   "source": [
    "print(f'x number of features: {x_feats}')\n",
    "print(f'y number of features: {y_feats}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb87787d-d140-4cfd-a3bb-131d4b31eeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitMLP(\n",
       "  (normalization_layer): Normalize()\n",
       "  (net): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): Linear(in_features=4128, out_features=100, bias=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (4): SiLU()\n",
       "    (5): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (6): SiLU()\n",
       "    (7): Linear(in_features=100, out_features=552, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = models.LitMLP(\n",
    "    in_channels=x_feats,\n",
    "    hidden_channels=100,\n",
    "    out_channels=y_feats\n",
    ")\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a08772-3965-46f4-b9f7-69d8915e8821",
   "metadata": {},
   "source": [
    "## Dataset creation and data loading mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fe32cf-92b3-47e3-8792-4eb2604c5c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MIT License\n",
      "# \n",
      "# Copyright (c) 2022 alxyok\n",
      "# \n",
      "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "# of this software and associated documentation files (the \"Software\"), to deal\n",
      "# in the Software without restriction, including without limitation the rights\n",
      "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "# copies of the Software, and to permit persons to whom the Software is\n",
      "# furnished to do so, subject to the following conditions:\n",
      "# \n",
      "# The above copyright notice and this permission notice shall be included in all\n",
      "# copies or substantial portions of the Software.\n",
      "# \n",
      "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "# SOFTWARE.\n",
      "\n",
      "import numpy as np\n",
      "import os.path as osp\n",
      "import pytorch_lightning as pl\n",
      "import torch\n",
      "from typing import Tuple, Union\n",
      "import sys\n",
      "\n",
      "sys.path.append(osp.abspath('..'))\n",
      "\n",
      "import config\n",
      "\n",
      "class FlattenedDataset(torch.utils.data.Dataset):\n",
      "    \n",
      "    def __init__(self, step: int) -> None:\n",
      "        super().__init__()\n",
      "        self.step = step\n",
      "        self.params = config.params[str(self.step)]['flattened']\n",
      "    \n",
      "    def __len__(self) -> int:\n",
      "        \n",
      "        return self.params['dataset_len']\n",
      "    \n",
      "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor]:\n",
      "        \n",
      "        shard_size = len(self) // self.params['num_shards']\n",
      "        fileidx = idx // shard_size\n",
      "        rowidx = idx % shard_size\n",
      "        \n",
      "        def _load(name: Union['x', 'y']) -> Tuple[torch.Tensor]:\n",
      "            main_path = osp.join(config.processed_data_path, f\"flattened-{self.step}\")\n",
      "            # data = np.memmap(\n",
      "            #     osp.join(main_path, name, f'{fileidx}.npy'), \n",
      "            #     dtype = self.params['dtype'],\n",
      "            #     mode='r',\n",
      "            #     shape=self.params[f'{name}_shape']\n",
      "            # )\n",
      "            data = np.load(osp.join(main_path, name, f'{fileidx}.npy'))\n",
      "            tensor = torch.squeeze(torch.tensor(data[rowidx, ...]))\n",
      "            return tensor\n",
      "        \n",
      "        x = _load('x')\n",
      "        y = _load('y')\n",
      "        \n",
      "        return x, y\n",
      "    \n",
      "\n",
      "class FlattenedDataModule(pl.LightningDataModule):\n",
      "    \n",
      "    def __init__(self, \n",
      "                 batch_size: int,\n",
      "                 num_workers: int) -> None:\n",
      "        \n",
      "        self.batch_size = batch_size\n",
      "        self.num_workers = num_workers\n",
      "        self.timestep = config.config['timestep']\n",
      "        super().__init__()\n",
      "        \n",
      "    def prepare_data(self) -> None:\n",
      "        pass\n",
      "    \n",
      "    def setup(self, stage: str) -> None:\n",
      "        dataset = FlattenedDataset(self.timestep)\n",
      "        length = len(dataset)\n",
      "        \n",
      "        self.train, self.val, self.test = torch.utils.data.random_split(\n",
      "            dataset,\n",
      "            [\n",
      "                int(length * .8), \n",
      "                int(length * .1), \n",
      "                int(length * .1)\n",
      "            ])\n",
      "    \n",
      "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
      "        \n",
      "        return torch.utils.data.DataLoader(\n",
      "            self.train, \n",
      "            batch_size=self.batch_size, \n",
      "            shuffle=True, \n",
      "            num_workers=self.num_workers)\n",
      "    \n",
      "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
      "        \n",
      "        return torch.utils.data.DataLoader(\n",
      "            self.val, \n",
      "            batch_size=self.batch_size, \n",
      "            num_workers=self.num_workers)\n",
      "    \n",
      "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
      "        \n",
      "        return torch.utils.data.DataLoader(\n",
      "            self.test, \n",
      "            batch_size=self.batch_size, \n",
      "            num_workers=self.num_workers)"
     ]
    }
   ],
   "source": [
    "!cat data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b240293-1b08-4d4d-9350-5f13cfc7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf4f5a-b347-41fd-81bf-95b2e020231d",
   "metadata": {},
   "source": [
    "* `batch_size` sets the number of element in a batch of data.\n",
    "* `num_workers` sets the number of workers the DataLoader can spawn to handle data loading and Dataset batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aa5af-10b4-43c7-bdf6-827bac3ca2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "cores = psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cef746e-d153-42fe-b4d8-85781c379b9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21337/3329079979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m datamodule = data.FlattenedDataModule(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# In CPU-only setup, make sure you have enough cores to handle the job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Otherwise this option will start a serious bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "datamodule = data.FlattenedDataModule(\n",
    "    batch_size=1024,\n",
    "    \n",
    "    # In CPU-only setup, make sure you have enough cores to handle the job\n",
    "    # Otherwise this option will start a serious bottleneck\n",
    "    num_workers=cores // 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4790a70-acaf-4252-83d3-f1ae6f862361",
   "metadata": {},
   "source": [
    "## Orchestrating the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "843cd867-8f72-4866-8601-e46ca0f8b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.tensorboard.TensorBoardLogger(\n",
    "    save_dir=config.logs_path,\n",
    "    name='flattened_mlp_logs',\n",
    "    log_graph=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631799c-cc96-4bca-8c2f-3a425fdf3022",
   "metadata": {},
   "source": [
    "All the training instrumentation is done by an object call the Trainer. You can fix parameters such as:\n",
    "* `max_epochs` unless an early stopping happens\n",
    "* `accelerator` type and `device` logical number\n",
    "\n",
    "Notably interesting: \n",
    "* `callbacks` to handle in-betweens\n",
    "* `gradient_clip_val` and `gradient_clip_algorithm` to setup the gradient clipping\n",
    "* `logger` to interface with loss and metrics logging\n",
    "* `resume_from_checkpoint` helps resuming a previously initiated training\n",
    "* `amp_backend` to switch to Nvidia Apex framework for Automatic Mixed Precision support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "093a4cd6-2526-4bf9-8ab6-b8e596687030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "cpu_trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd78404-a02b-48aa-bbfb-7a075ebe2561",
   "metadata": {},
   "source": [
    "Training CPU is a one-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bc17ca0-4387-4cb9-9622-2ba1f482d17f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                | Type       | Params\n",
      "---------------------------------------------------\n",
      "0 | normalization_layer | Normalize  | 0     \n",
      "1 | net                 | Sequential | 488 K \n",
      "---------------------------------------------------\n",
      "488 K     Trainable params\n",
      "0         Non-trainable params\n",
      "488 K     Total params\n",
      "1.955     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|â–‹         | 63/954 [01:56<27:24,  1.85s/it, loss=0.948, v_num=6, train_loss=1.190]  "
     ]
    }
   ],
   "source": [
    "cpu_trainer.fit(model=mlp, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b563c5-dce8-403f-acd5-b8b8a58c706b",
   "metadata": {},
   "source": [
    "Never forget to test. The handy thing with the `Trainer` is, if a `.test()` is called somewhere at runtime, once a `SIGTERM` is thrown by the runtime such as a `KeyboardInterruptError`, it gets caught by Lightning, which tries to then run the test anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7e8cc-3bc4-45fa-a30d-b7b71ac83166",
   "metadata": {},
   "source": [
    "## Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf5bcd-16b3-4425-8c1b-d47ef2fc3f23",
   "metadata": {},
   "source": [
    "A Note on GPU training. We naturally assume that GPU is better than CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "Now let's go single-node multi-GPU. The same model will be pushed to all available devices, each of which will\n",
    "1. Perform forward pass with its specific batch of data\n",
    "2. Compute the loss and perform backward pass including weights update\n",
    "4. The weights are then collected are synchronized across all devices for next pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46384195-8e49-4f5d-9742-bdde1703f334",
   "metadata": {},
   "source": [
    "#### DDP Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393daca6-38b3-4be4-9f87-e3b40402d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf0f764c-03b4-4e9d-bba8-59d80ec576b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "gpu_trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    max_epochs=1,\n",
    "    logger=logger,\n",
    "    deterministic=True,\n",
    "    # amp_backend='apex'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "501862a9-d0a0-415d-90c9-33f9807fec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   7%|â–‹         | 63/954 [02:12<31:17,  2.11s/it, loss=0.948, v_num=6, train_loss=1.190]"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 207, in _wrapped_function\n    self._worker_setup(process_idx)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 217, in _worker_setup\n    self.cluster_environment, self.torch_distributed_backend, self.global_rank, self.world_size\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 388, in init_dist_connection\n    torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\nRuntimeError: Address already in use\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21798/2543308589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpu_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgpu_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         )\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# reset optimizers, since main process is never used for training and thus does not have a valid optim state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(self, function, return_result, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mreturn_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_result\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_result\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    228\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[1;32m    229\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 207, in _wrapped_function\n    self._worker_setup(process_idx)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 217, in _worker_setup\n    self.cluster_environment, self.torch_distributed_backend, self.global_rank, self.world_size\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 388, in init_dist_connection\n    torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py\", line 576, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 229, in _env_rendezvous_handler\n    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/distributed/rendezvous.py\", line 158, in _create_c10d_store\n    hostname, port, world_size, start_daemon, timeout, multi_tenant=True\nRuntimeError: Address already in use\n"
     ]
    }
   ],
   "source": [
    "gpu_trainer.fit(model=mlp, datamodule=datamodule)\n",
    "gpu_trainer.test(model=mlp, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278d105-9dd9-4a76-8abb-abc77f7e346c",
   "metadata": {},
   "source": [
    "#### Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1aa2c-5ea9-4726-8f49-e4391ebcc295",
   "metadata": {},
   "source": [
    "With a simple change in the Trainer options, you can rely on horovod backend to perform the computations. Prallelism is achieved by SPMD with MPI: one process per GPU, and collective computing is made by process of rank 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67b3c6-ebdb-4780-be9d-ab6a8768eabe",
   "metadata": {},
   "source": [
    "## Model Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280a2db-6f67-41b7-85b6-d4a44af262de",
   "metadata": {},
   "source": [
    "You should really go for model parallelism starting at 500M parameters. No material on that, just know that it exists and it is complex. Lightning comes standard with a series of distrubtion strategies, each with a specific implementation related to the network that first introduced it.\n",
    "\n",
    "Refer to the Doc for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1df06b-1cd5-4f84-8bbf-03eaa6fc4937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
