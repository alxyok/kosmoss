{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c15fb0b3-888c-4713-b358-2c8961b177b1",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 alxyok\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40589b09-ef34-4bd9-8540-1beadc29aa6d",
   "metadata": {},
   "source": [
    "****************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd6e24d-405a-46fa-aa5b-1956cad4e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the root of this bootcamp\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "sys.path.append(osp.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# 1. Scaling UP (mono-node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf5bcd-16b3-4425-8c1b-d47ef2fc3f23",
   "metadata": {},
   "source": [
    "A Note on GPU training. We naturally assume that GPU is better than CPU, but it really depends on the workflow. You need to saturate the GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "Now let's go single-node multi-GPU. The same model will be pushed to all available devices, each of which will\n",
    "1. Perform forward pass with its specific batch of data\n",
    "2. Compute the loss and perform backward pass including weights update\n",
    "4. The weights are then collected are synchronized across all devices for next pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fc516-0a24-450c-bd45-0840f9a8b9b8",
   "metadata": {},
   "source": [
    "#### **Cleanup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181424c-68e4-4a06-9361-2464b51f0525",
   "metadata": {},
   "source": [
    "You might need to clean up your ghost runs if something fails and break the training logic. You can do this one of two ways:\n",
    "* If run inside the same PID as the training from a `python train.py`:\n",
    "<code>\n",
    "import gc, torch; gc.collect(); torch.cuda.empty_cache()\n",
    "</code>\n",
    "* Otherwise, try to kill the job still running on the GPU, by get the ghost job's PID with the command `nvitop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a434084d-9827-458a-8835-3281f4f04803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 16 14:06:27 2022\n",
      "╒═════════════════════════════════════════════════════════════════════════════╕\n",
      "│ NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     │\n",
      "├───────────────────────────────┬──────────────────────┬──────────────────────┤\n",
      "│ GPU  Name        Persistence-M│ Bus-Id        Disp.A │ Volatile Uncorr. ECC │\n",
      "│ Fan  Temp  Perf  Pwr:Usage/Cap│         Memory-Usage │ GPU-Util  Compute M. │\n",
      "╞═══════════════════════════════╪══════════════════════╪══════════════════════╡\n",
      "│\u001b[32m   0  Tesla P100-PCIE...  Off  \u001b[0m│\u001b[32m 00000000:00:04.0 Off \u001b[0m│\u001b[32m                    0 \u001b[0m│\n",
      "│\u001b[32m MAX   41C    P0    27W / 250W \u001b[0m│\u001b[32m      2MiB / 16281MiB \u001b[0m│\u001b[32m      0%      Default \u001b[0m│\n",
      "├───────────────────────────────┼──────────────────────┼──────────────────────┤\n",
      "│\u001b[32m   1  Tesla P100-PCIE...  Off  \u001b[0m│\u001b[32m 00000000:00:05.0 Off \u001b[0m│\u001b[32m                    0 \u001b[0m│\n",
      "│\u001b[32m MAX   42C    P0    27W / 250W \u001b[0m│\u001b[32m      2MiB / 16281MiB \u001b[0m│\u001b[32m      0%      Default \u001b[0m│\n",
      "╘═══════════════════════════════╧══════════════════════╧══════════════════════╛\n",
      "\u001b[1m\u001b[36m[ CPU: █▉ 6.3%                            ]\u001b[0m  \u001b[1m( Load Average:  0.97  1.94  4.21 )\u001b[0m\n",
      "\u001b[1m\u001b[35m[ MEM: █▊ 5.8%                            ]\u001b[0m  \u001b[1m\u001b[34m[ SWP: ▏ 0.0%                     ]\u001b[0m\n",
      "\n",
      "╒══════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Processes:                                               \u001b[1m\u001b[35mjupyter\u001b[0m\u001b[1m@\u001b[0m\u001b[1m\u001b[32malx-devel-2\u001b[0m │\n",
      "│ GPU     PID      USER  GPU-MEM %SM  %CPU  %MEM  TIME  COMMAND                │\n",
      "╞══════════════════════════════════════════════════════════════════════════════╡\n",
      "│  No running processes found                                                  │\n",
      "╘══════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "!nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c8c407-77c8-4996-8b61-02931a536e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: (99999999): No such process\n"
     ]
    }
   ],
   "source": [
    "# Replace in the command below the PID=99999999 by the PID number produced by nvitop\n",
    "!sudo kill -15 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0b0bb-8d00-4755-9627-35400021164e",
   "metadata": {},
   "source": [
    "## 1.1. Achieving Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcddaec-41af-48e4-abc1-049ddd51771d",
   "metadata": {},
   "source": [
    "#### **1.1.1. Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b1f56-ceba-420d-b0c6-a15405b7ff7b",
   "metadata": {},
   "source": [
    "DP consists of parallelizing the model, and training each instance of the model with a different mini-batch of data of size `batch_size // num_parallel_instances`. Each model will converge differently on its mini-batch, so the weights are collected and usually averaged after `p` batches, then synchronized with all instances for the next round of passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4c97-7b50-4a5c-990e-209cc0db5635",
   "metadata": {},
   "source": [
    "#### **0. Over CPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdce8d-9047-4185-b688-5af9330b671f",
   "metadata": {},
   "source": [
    "Let's launch a reference training on CPU. Take a look at the `_trainup_cpu.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81332c0-32d3-47f8-9912-dc4db5207038",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python _trainup_cpu.py --batch-size 512 \\\n",
    "                        --num-processes 2 > ../logs/trainup_cpu.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18532a-4a29-4ffd-9b06-c60f636cc627",
   "metadata": {},
   "source": [
    "#### **1. Launching a training on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52ae8c9-f0ec-4103-b18d-1c86951af0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# MIT License\n",
      "#\n",
      "# Copyright (c) 2022 alxyok\n",
      "#\n",
      "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "# of this software and associated documentation files (the \"Software\"), to deal\n",
      "# in the Software without restriction, including without limitation the rights\n",
      "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "# copies of the Software, and to permit persons to whom the Software is\n",
      "# furnished to do so, subject to the following conditions:\n",
      "#\n",
      "# The above copyright notice and this permission notice shall be included in all\n",
      "# copies or substantial portions of the Software.\n",
      "#\n",
      "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "# SOFTWARE.\n",
      "\n",
      "import numpy as np\n",
      "import os\n",
      "import os.path as osp\n",
      "import psutil\n",
      "import pytorch_lightning as pl\n",
      "import torch\n",
      "from typing import Union\n",
      "import sys\n",
      "\n",
      "def main(strategy: Union['ddp', 'horovod'],\n",
      "         batch_size: int,\n",
      "         gpus: int,\n",
      "         num_nodes: int) -> None:\n",
      "    \n",
      "    sys.path.append(osp.abspath('..'))\n",
      "\n",
      "    import config\n",
      "    import data\n",
      "    import models\n",
      "\n",
      "    pl.seed_everything(42, workers=True)\n",
      "    \n",
      "    step = config.config['timestep']\n",
      "    params = config.params[str(step)]['flattened']\n",
      "\n",
      "    x_feats = params['x_shape'][-1]\n",
      "    y_feats = params['y_shape'][-1]\n",
      "\n",
      "    mlp = models.LitMLP(\n",
      "        in_channels=x_feats,\n",
      "        hidden_channels=100,\n",
      "        out_channels=y_feats,\n",
      "        lr=1e-4,\n",
      "    )\n",
      "\n",
      "    cores = psutil.cpu_count(logical=False)\n",
      "    datamodule = data.FlattenedDataModule(\n",
      "        # Adjusting the total batch size\n",
      "        batch_size=batch_size // (num_nodes * gpus),\n",
      "        num_workers=cores\n",
      "    )\n",
      "\n",
      "    logger = pl.loggers.tensorboard.TensorBoardLogger(\n",
      "        save_dir=config.logs_path,\n",
      "        name='flattened_mlp_logs',\n",
      "        log_graph=True\n",
      "    )\n",
      "\n",
      "    gpu_trainer = pl.Trainer(\n",
      "        strategy=strategy,\n",
      "        gpus=gpus,\n",
      "        max_epochs=1,\n",
      "        logger=logger,\n",
      "        deterministic=True,\n",
      "        num_sanity_val_steps=0,\n",
      "        num_nodes=num_nodes\n",
      "        # Uncomment if you want to use Nvidia's own implementation of AMP called APEX\n",
      "        # amp_backend='apex',\n",
      "        \n",
      "        # Useful if you want to profile your training for debug purposes\n",
      "        # profiler=\"simple\"\n",
      "    )\n",
      "    gpu_trainer.fit(model=mlp, datamodule=datamodule)\n",
      "    gpu_trainer.test(model=mlp, datamodule=datamodule)\n",
      "\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    import argparse\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--strategy', \n",
      "                        type=str,\n",
      "                        help='Data Distributed Strategy', \n",
      "                        default='ddp')\n",
      "    parser.add_argument('--batch-size', \n",
      "                        type=int,\n",
      "                        help='Total batch size', \n",
      "                        default=512)\n",
      "    parser.add_argument('--gpus', \n",
      "                        type=int,\n",
      "                        help='Number of GPUs to accelerate over', \n",
      "                        default=1)\n",
      "    parser.add_argument('--num-nodes', \n",
      "                        type=int,\n",
      "                        help='Number of nodes to accelerate over', \n",
      "                        default=1)\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    main(\n",
      "        args.strategy, \n",
      "        args.batch_size, \n",
      "        args.gpus,\n",
      "        args.num_nodes\n",
      "    )"
     ]
    }
   ],
   "source": [
    "!cat _trainup_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0164d-921b-450a-886e-24e0031dd942",
   "metadata": {},
   "source": [
    "Let's launch the training with 2 nodes and 1 GPU/node. Since we're on a single node, each node designates an independent process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f1fd8-16f2-4e20-b97d-9b5b6d078338",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python _trainup_gpu.py --batch-size 512 \\\n",
    "                       --lr=1e-4 \\\n",
    "                       --strategy 'ddp' \\\n",
    "                       --gpus 1 \\\n",
    "                       --num-nodes 2 > ../logs/trainup_gpu_ddp.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa18a54-6e1a-457e-942f-2a1707a93f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i**2) for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278d105-9dd9-4a76-8abb-abc77f7e346c",
   "metadata": {},
   "source": [
    "#### **1.1.2. Horovod**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1aa2c-5ea9-4726-8f49-e4391ebcc295",
   "metadata": {},
   "source": [
    "With a simple change in the Trainer options, you can rely on horovod backend to perform the computations. Prallelism is achieved by SPMD with MPI: one process per GPU potentially distributed accross multiple nodes, and collective computing is made by process of rank 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc42e3d-483e-408e-a67b-bca8af394799",
   "metadata": {},
   "source": [
    "No need to adjust the learning rate `lr` this time, horovod takes care of that underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987074b-ddc3-454b-834c-4bd86de75025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python _trainup_gpu.py --batch-size 512 \\\n",
    "                       --strategy 'horovod' \\\n",
    "                       --gpus 1 \\\n",
    "                       --num-nodes 2 > ../logs/trainup_gpu_horovod.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67b3c6-ebdb-4780-be9d-ab6a8768eabe",
   "metadata": {},
   "source": [
    "## **1.2. A Note on Model Parallelism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280a2db-6f67-41b7-85b6-d4a44af262de",
   "metadata": {},
   "source": [
    "You should really go for model parallelism starting at 500M parameters. No material on that, just know that it exists and it is complex subject that would require an entire session. Lightning comes standard with a series of distrubtion strategies, each with a specific implementation related to the network that first introduced it.\n",
    "\n",
    "Refer to the Doc for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1df06b-1cd5-4f84-8bbf-03eaa6fc4937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
