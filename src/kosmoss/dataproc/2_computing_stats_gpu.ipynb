{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Computing basic Stats with the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0db00-d0b0-4b51-a603-ced9f855d47d",
   "metadata": {},
   "source": [
    "There are many framework in the tech stack to push for gpu-based processing including the *Rapids.ai* collection of tools developed partly by **Nvidia** and an Open Source project, but until the hardware converges to the unified memory, this initiative has very limited use-cases in the overall scientific ML algorithmic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d4852-2a58-4df9-a537-daca1521fb69",
   "metadata": {},
   "source": [
    "We'll explore those limits in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330287a5-cda4-4368-b64b-00288ba71d3a",
   "metadata": {},
   "source": [
    "Let's first load the data from NumPy files using Dask lazy loading. For the sake of the demonstration, let's load only `x` for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e8a47f-b945-4cea-8157-80aa7fd9bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kosmoss import CONFIG, PROCESSED_DATA_PATH\n",
    "from kosmoss.utils import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a82a516-48d1-471b-9b08-758be80e258f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "step = CONFIG['timestep']\n",
    "num_workers = CONFIG['num_workers']\n",
    "\n",
    "features_path = osp.join(PROCESSED_DATA_PATH, f'features-{step}')\n",
    "\n",
    "x = da.from_npy_stack(osp.join(features_path, 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20dbbb3-a7c1-489a-a8f5-656f58ce3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def load_and_push_gpu(x: da.Array) -> None:\n",
    "    \n",
    "    with cp.cuda.Device(0):\n",
    "        \n",
    "        # Loading the data into CPU memory\n",
    "        x_ = cp.array(x.compute(num_workers=num_workers))\n",
    "        \n",
    "        # Pushing the data into GPU memory\n",
    "        x_mean_gpu = cp.mean(x_, axis=0)\n",
    "        \n",
    "        # Retrieving the data from GPU to CPU\n",
    "        x_mean_cpu_back = cp.asnumpy(x_mean_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9eee22-eae5-437c-aec9-595e48a381c2",
   "metadata": {},
   "source": [
    "You can CPU and GPU memory grow and compute surface being utilized by running `htop` and `watch -n 1 nvitop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ec4cd8-de09-4527-99dd-1deb36e7df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20628.32 ms\n"
     ]
    }
   ],
   "source": [
    "load_and_push_gpu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496af216-7f3a-4dd9-8b12-38f92e9c8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def multi_cpu_stats_compute_reminder(x: da.Array) -> None:\n",
    "    x_mean_multi_cpu = da.mean(x, axis=0).compute(num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352998b5-2ce3-4625-bada-ee6b7ba15def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609.86 ms\n"
     ]
    }
   ],
   "source": [
    "multi_cpu_stats_compute_reminder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4687c1-0229-46f2-a9b7-7ca8a40e8d02",
   "metadata": {},
   "source": [
    "No Comment.\n",
    "\n",
    "Actually yes. Three. \n",
    "\n",
    "* It starts to become interesting when the original dataset size grows. But since the GPU is really memory bound, it rapidly becomes a bottleneck. Waiting for a definitive unified memory architecture.\n",
    "* If you want to overcome the memory bottleneck, you have to take of all the boiler plate, which is why you turn to framework in the first place is to avoid this.\n",
    "* Using GPUDirect could improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5da651-faf2-41a1-896b-e62f4f99cd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
