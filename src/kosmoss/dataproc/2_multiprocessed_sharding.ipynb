{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Multiprocessed Sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324f441-2699-4d4c-9e0d-081017d241ce",
   "metadata": {},
   "source": [
    "On the previous notebook, we've seen how to leverage Dask/Xarray to shard the data in a multi-threaded fashion.\n",
    "\n",
    "* We can also perform this with MPI with multiple processed, reading the same HDF5 input file with [the `h5py` library](https://docs.h5py.org/en/stable/quick.html#quick), example below\n",
    "* If we wanted to perform write concurrency, `h5py` can be compiled with parallel processing in mind with the proper flags. [Follow the guide](https://docs.h5py.org/en/stable/mpi.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb98219-6d6a-44d4-b3aa-f541d0a6a8f3",
   "metadata": {},
   "source": [
    "## Debugging the script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a6049-f487-4ea2-9ea2-4fd75c15c739",
   "metadata": {},
   "source": [
    "Open the file named `shard_parallel.py` and debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33011fa5-a07e-4a00-abb9-1f857aa61db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import h5py\n",
      "from mpi4py import MPI\n",
      "import numpy as np\n",
      "import os.path as osp\n",
      "\n",
      "from kosmoss import PROCESSED_DATA_PATH\n",
      "\n",
      "def main() -> None:\n",
      "\n",
      "    # Being multiprocessed, threads can't open the same file for concurrency-related issues\n",
      "    # So we don't read the timestep variable from the config.yaml file and instead, have to fix the value\n",
      "    timestep = 1000\n",
      "    h5_path = osp.join(PROCESSED_DATA_PATH, f'features-{timestep}.h5')\n",
      "    out_dir = osp.join(PROCESSED_DATA_PATH, f\"features-{timestep}\")\n",
      "    os.makedirs(out_dir, exist_ok=True)\n",
      "\n",
      "    # The MPI Rank uniquely identify each process\n",
      "    rank = MPI.COMM_WORLD.rank\n",
      "    print(f'worker of rank {rank} started.')\n",
      "\n",
      "    # Each process will produce 53 files\n",
      "    for subidx in np.arange(53):\n",
      "        \n",
      "        print(f'processing slice {subidx} for rank {rank}.')\n",
      "        \n",
      "        # Each file holding 4800 records\n",
      "        start = rank * 53 * 2 ** 4 + subidx * 4800\n",
      "        end = start + 4800\n",
      "\n",
      "        with h5py.File(h5_path, 'r') as feats:\n",
      "\n",
      "            # Give the output file a unique name to avoid overriting\n",
      "            name = (rank + 1) * (subidx + 1)\n",
      "            sharded_path = osp.join(out_dir, f'features-{name}.h5')\n",
      "            with h5py.File(sharded_path, 'w') as sharded:\n",
      "\n",
      "                sharded.create_dataset(\"/x\", data=feats['/x'][start:end])\n",
      "                sharded.create_dataset(\"/y\", data=feats['/y'][start:end])\n",
      "                sharded.create_dataset(\"/edge\", data=feats['/edge'][start:end])\n",
      "                \n",
      "    print(f'ending session for worker of rank {rank}.')\n",
      "    \n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "    \n",
      "    main()"
     ]
    }
   ],
   "source": [
    "!cat shard_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acc7c9-540b-4128-9249-d23e22995abe",
   "metadata": {},
   "source": [
    "## Executing the script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c885eae-1af9-438b-997e-eee9d2a7b163",
   "metadata": {},
   "source": [
    "For some reason, `mpiexec` doesn't fancy being called from within a notebook?\n",
    "You'll have to execute the follow command in a terminal manually from within the script directory.\n",
    "\n",
    "Run `htop` on one terminal and run the following block in another one:\n",
    "<code>\n",
    "mpiexec -n 16 python shard_parallel.py\n",
    "</code>\n",
    "This will launch the same Python script on 16 MPI nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991dd9ac-9bb8-49b2-bc60-611f76f74e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
