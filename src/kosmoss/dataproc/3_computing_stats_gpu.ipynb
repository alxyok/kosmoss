{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Computing basic Stats with the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0db00-d0b0-4b51-a603-ced9f855d47d",
   "metadata": {},
   "source": [
    "Instead of computing the basic stats on CPU, we COULD leverage GPU and CUDA processing...\n",
    "\n",
    "There are a few framework in the tech stack to push for GPU-based processing including the **Rapids.ai** collection of tools developed partly by Nvidia and an Open Source project, and dedicated to data analytics and classic, statistics-driven ML. It's pairable with Dask and XGBoost for distributed ML. A few components of interest, [full list here](https://docs.rapids.ai/api):\n",
    "\n",
    "* `cuDF`, a library for DataFrame manipulation built on Arrow\n",
    "* `cuML`, a library with ML algs and ML primitives\n",
    "* `cuGraph` to interface with the famous fully Pythonic NetworkX library\n",
    "\n",
    "**...BUT,** until the hardware converges to the unified memory, this initiative has very limited use-cases in the overall scientific ML algorithmic.\n",
    "\n",
    "We'll explore those limits in this notebook with the [CuPy framework](https://cupy.dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96084b6f-ecf1-497d-994a-149ccca46b38",
   "metadata": {},
   "source": [
    "## Push to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097639c-6787-4319-a0c0-37413c2735a6",
   "metadata": {},
   "source": [
    "CuPy is an open-source array library for GPU-accelerated computing with Python. \n",
    "\n",
    "CuPy utilizes CUDA Toolkit libraries including cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL to make full use of the GPU architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e8a47f-b945-4cea-8157-80aa7fd9bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kosmoss import CONFIG, PROCESSED_DATA_PATH\n",
    "from kosmoss.utils import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a82a516-48d1-471b-9b08-758be80e258f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "step = CONFIG['timestep']\n",
    "num_workers = CONFIG['num_workers']\n",
    "features_path = osp.join(PROCESSED_DATA_PATH, f'features-{step}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330287a5-cda4-4368-b64b-00288ba71d3a",
   "metadata": {},
   "source": [
    "Let's first load the data from NumPy files using Dask lazy loading. For the sake of the demonstration, let's load only `x` for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b333d52-e44a-40d1-b140-3e9b39f10f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.from_npy_stack(osp.join(features_path, 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20dbbb3-a7c1-489a-a8f5-656f58ce3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def load_and_push_gpu(x: da.Array) -> None:\n",
    "    \n",
    "    with cp.cuda.Device(0):\n",
    "        \n",
    "        # Loading the data into CPU memory\n",
    "        x_ = cp.array(x.compute(num_workers=num_workers))\n",
    "        \n",
    "        # Pushing the data into GPU memory\n",
    "        x_mean_gpu = cp.mean(x_, axis=0)\n",
    "        \n",
    "        # Retrieving the data from GPU to CPU\n",
    "        x_mean_cpu_back = cp.asnumpy(x_mean_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9eee22-eae5-437c-aec9-595e48a381c2",
   "metadata": {},
   "source": [
    "You can CPU and GPU memory grow and compute surface being utilized by running `htop` and `watch -n 1 nvitop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec4cd8-de09-4527-99dd-1deb36e7df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_push_gpu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3d0b1-0cea-463d-bc37-f156a305d411",
   "metadata": {},
   "source": [
    "## Reminder on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496af216-7f3a-4dd9-8b12-38f92e9c8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def multi_cpu_stats_compute_reminder(x: da.Array) -> None:\n",
    "    x_mean_multi_cpu = da.mean(x, axis=0).compute(num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352998b5-2ce3-4625-bada-ee6b7ba15def",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cpu_stats_compute_reminder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4687c1-0229-46f2-a9b7-7ca8a40e8d02",
   "metadata": {},
   "source": [
    "No Comment.\n",
    "\n",
    "Actually yes. Three. \n",
    "\n",
    "* It starts to become interesting when the original dataset size grows. But since the GPU is really memory bound, it rapidly becomes a bottleneck. Waiting for a definitive unified memory architecture.\n",
    "* If you want to overcome the memory bottleneck, you have to take of all the boiler plate, which is why you turn to framework in the first place is to avoid this.\n",
    "* Using GPUDirect could improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5da651-faf2-41a1-896b-e62f4f99cd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
