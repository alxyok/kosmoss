{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building graphs with Metaflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3575e-e0f0-41f4-b98d-6e795d7afa24",
   "metadata": {},
   "source": [
    "[Metaflow](https://docs.metaflow.org/metaflow/basics) is a great framework designed by Netflix for managing **data related workflows**. \n",
    "\n",
    "It can perform multiprocessed tasks threrefore bypassing the [Python's GIL restrictions](https://docs.python.org/3/glossary.html#term-global-interpreter-lock) by leveraging the subprocess (separate Python interpreter) in a still very Pythonic dev angle. Unlike with MPI-based programs, processes can also share data through superclass attributes. \n",
    "\n",
    "It tends to be compute intensive on the CPU, but in a host-dedicated environment, it's still a handy tool.\n",
    "\n",
    "We will demonstrate its ease-of-use on a simple example: building the graphs from the previously processed featured data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966ede38-59e6-4770-8735-28effa83b592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/bash \n",
      "\n",
      "export MAX_WORKERS=$(python -c \"import psutil; print(psutil.cpu_count(logical=False))\")\n",
      "\n",
      "# Usually, you should enable pylint, really\n",
      "# But because PyTorch generates errors on its own, we'll simplify by just disabling it\n",
      "# Our code is clean though ;)\n",
      "USERNAME='mluser' python flows.py --no-pylint \\\n",
      "    run \\\n",
      "        --max-num-splits 7000 \\\n",
      "        --max-workers ${MAX_WORKERS} >> ${HOME}/.kosmoss/logs/build_graphs.stdout"
     ]
    }
   ],
   "source": [
    "!cat build_graphs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41954438-27ea-44a7-b8e8-99458eee7b30",
   "metadata": {},
   "source": [
    "Open the `flows.py` file and debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58771be-76a5-436f-8663-0747321bae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from metaflow import FlowSpec, Parameter, step\n",
      "import os\n",
      "import os.path as osp\n",
      "import shutil\n",
      "\n",
      "class BuildGraphsFlow(FlowSpec):\n",
      "    \n",
      "    # In addition to the standard class properties...\n",
      "    PROCESSED_DATA_PATH = osp.join(os.environ['HOME'], \".kosmoss\", \"data\", \"processed\")\n",
      "\n",
      "    # ...you can just add parameters to be read from the command line\n",
      "    timestep = Parameter('timestep', help='Temporal sampling step', default=1000)\n",
      "    num_shards = Parameter('num_shards', help='Number of shards', default=3392)\n",
      "    dtype = Parameter('dtype', help=\"NumPy's dtype\", default='float32')\n",
      "    x_shape = Parameter('x_shape', help='Shape for x', default=(160, 136, 20))\n",
      "    y_shape = Parameter('y_shape', help='Shape for y', default=(160, 138, 4))\n",
      "    edge_shape = Parameter('edge_shape', help='Shape for edge', default=(160, 137, 27))\n",
      "        \n",
      "    @step\n",
      "    def start(self):\n",
      "        \"\"\"\n",
      "        Create the constants for the rest of the Flow.\n",
      "        \"\"\"\n",
      "        \n",
      "        import numpy as np\n",
      "        \n",
      "        # Each 'common' step can store a shared property to all steps\n",
      "        # Start is common here, and we branch on next step\n",
      "        # More info https://docs.metaflow.org/metaflow/basics#branch\n",
      "        self.out_dir = osp.join(self.PROCESSED_DATA_PATH, f\"graphs-{self.timestep}\")\n",
      "        if osp.isdir(self.out_dir):\n",
      "            shutil.rmtree(self.out_dir)\n",
      "        os.makedirs(self.out_dir)\n",
      "        \n",
      "        # To launch in thread in parallel, just call the next step over an attribute's list\n",
      "        self.shard = np.arange(self.num_shards)\n",
      "        self.next(self.build_graphs, foreach=\"shard\")\n",
      "                  \n",
      "                  \n",
      "    @step\n",
      "    def build_graphs(self):\n",
      "        \"\"\"\n",
      "        1) Load the raw data.\n",
      "        2) Extract features for x and y.\n",
      "        3) Sequentially iterate the sharded subset sample to create and save the graph for each row.\n",
      "        \"\"\"\n",
      "        \n",
      "        import numpy as np\n",
      "        import torch\n",
      "        import torch.nn.functional as F\n",
      "        import torch_geometric as pyg\n",
      "        from typing import Union\n",
      "        \n",
      "        main_dir = osp.join(self.PROCESSED_DATA_PATH, f\"features-{self.timestep}\")\n",
      "        \n",
      "        def load(name: Union['x', 'y', 'edge']) -> torch.Tensor:\n",
      "            return torch.tensor(\n",
      "                np.lib.format.open_memmap(\n",
      "                    mode='r', \n",
      "                    dtype=self.dtype, \n",
      "                    filename=osp.join(main_dir, name, f\"{self.input}.npy\"), \n",
      "                    shape=getattr(self, f'{name}_shape')))\n",
      "                \n",
      "        x, y, edge = load(\"x\"), load(\"y\"), load(\"edge\")\n",
      "        \n",
      "        data_list = []\n",
      "        \n",
      "        # Build the both-ways connectivity matrix\n",
      "        directed_idx = np.array([[*range(1, 138)], [*range(137)]])\n",
      "        undirected_idx = np.hstack((\n",
      "            directed_idx, \n",
      "            directed_idx[[1, 0], :]\n",
      "        ))\n",
      "        undirected_idx = torch.tensor(undirected_idx, dtype=torch.long)\n",
      "        \n",
      "        # Iterate over the rows of the sharded file\n",
      "        for idx in range(len(x)):\n",
      "            \n",
      "            # For each element, simply extract:\n",
      "            # The nodes features (input x, output y)\n",
      "            x_ = torch.squeeze(x[idx, ...])\n",
      "            y_ = torch.squeeze(y[idx, ...])\n",
      "            \n",
      "            # The edge attributes\n",
      "            edge_ = torch.squeeze(edge[idx, ...])\n",
      "\n",
      "            # Build a graph for that element\n",
      "            data = pyg.data.Data(x=x_, edge_attr=edge_, edge_index=undirected_idx, y=y_,)\n",
      "            \n",
      "            # Append the data to a list\n",
      "            data_list.append(data)\n",
      "            \n",
      "        # Save the list with torch.save()\n",
      "        out_path = osp.join(self.out_dir, f\"data-{self.input}.pt\")\n",
      "        torch.save(data_list, out_path)\n",
      "        \n",
      "        self.next(self.join)\n",
      "        \n",
      "        \n",
      "    @step\n",
      "    def join(self, inputs):\n",
      "        \"\"\"\n",
      "        Join the parallel branches.\n",
      "        \"\"\"\n",
      "        \n",
      "        self.next(self.end)\n",
      "        \n",
      "        \n",
      "    @step\n",
      "    def end(self):\n",
      "        \"\"\"\n",
      "        End the flow.\n",
      "        \"\"\"\n",
      "        \n",
      "        pass\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    BuildGraphsFlow()"
     ]
    }
   ],
   "source": [
    "!cat flows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b7c0bf-542b-4b3c-93d7-2af9c102eee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\u001b[1mMetaflow 2.5.2\u001b[0m\u001b[35m\u001b[22m executing \u001b[0m\u001b[31m\u001b[1mBuildGraphsFlow\u001b[0m\u001b[35m\u001b[22m\u001b[0m\u001b[35m\u001b[22m for \u001b[0m\u001b[31m\u001b[1muser:mluser\u001b[0m\u001b[35m\u001b[22m\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[22mCreating local datastore in current directory (/home/agiorkallos/kosmoss/src/kosmoss/dataproc/.metaflow)\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[35m\u001b[22mValidating your flow...\u001b[K\u001b[0m\u001b[35m\u001b[22m\u001b[0m\n",
      "\u001b[32m\u001b[1m    The graph looks good!\u001b[K\u001b[0m\u001b[32m\u001b[1m\u001b[0m\n",
      "\u001b[1m    Step failure\u001b[0m\u001b[22m:\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m    Step \u001b[0m\u001b[31m\u001b[1mbuild_graphs\u001b[0m\u001b[22m (task-id 7) failed.\u001b[K\u001b[0m\u001b[22m\u001b[0m\n",
      "\u001b[22m\u001b[K\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bash build_graphs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252f92e-2e23-44a1-9962-462819e034d3",
   "metadata": {},
   "source": [
    "The neat thing with Metaflow is that it registers everything in a namespace, and centralizes the logs and artifacts produced for each run. \n",
    "\n",
    "This data is then viewable with the commands below. Everything is Python-scriptable, which is a huge advantage.\n",
    "\n",
    "We launched the run with the `USERNAME` set at `'mluser'` so everything is stored under that namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec63e66-1b40-4b8e-b739-4650cf3757b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/.kosmoss/data/processed/graphs-1000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14456/189665460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisk_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROCESSED_DATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"graphs-{CONFIG['timestep']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mused\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mdisk_usage\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0;34m'free'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mamount\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfree\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \"\"\"\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatvfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0mfree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_bavail\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_frsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_blocks\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_frsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/.kosmoss/data/processed/graphs-1000'"
     ]
    }
   ],
   "source": [
    "from kosmoss import CONFIG, PROCESSED_DATA_PATH\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "_, used, _ = shutil.disk_usage(osp.join(PROCESSED_DATA_PATH, f\"graphs-{CONFIG['timestep']}\"))\n",
    "used // 2 ** 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f766c2-1ec2-4efd-87d7-88950728eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow, namespace\n",
    "from pprint import pprint\n",
    "\n",
    "namespace('user:mluser')\n",
    "flow = Flow('BuildGraphsFlow')\n",
    "runs = list(flow)\n",
    "run0 = runs[0]\n",
    "run0.data.name\n",
    "\n",
    "pprint(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867523d-7b82-47c6-b153-62b1ab9ae6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolated last Run\n",
    "run = Flow('BuildGraphsFlow').latest_run\n",
    "\n",
    "# Get Steps from that Run\n",
    "steps = list(run.steps())\n",
    "pprint(steps)\n",
    "\n",
    "# Isolate Tasks from the Start Step\n",
    "start_tasks = list(steps[-1].tasks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f16fa0-6c82-40f5-a075-306106216b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrieve the list of artifacts registered at the Start Step\n",
    "start_artifacts = start_tasks[0].artifacts\n",
    "list(start_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3f4a4-43e8-45e3-a449-3e39f19e500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_artifacts.num_shards.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2465e10-12fd-4710-a665-01976f2cbc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
