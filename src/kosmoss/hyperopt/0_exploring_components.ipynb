{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Ray.io optimization framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a16b2-58ac-467e-bcb7-da78fe8b726c",
   "metadata": {},
   "source": [
    "[Ray.io](https://docs.ray.io/en/latest/ray-overview/index.html) is a framework developed to scale compute-intensive Python workload. It relies on many components dedicated among which, notoriously:\n",
    "\n",
    "* Ray Core to scale general-purpose Python workflows\n",
    "* Ray Train for scaling DL-models training\n",
    "* Ray Serve for scaling models inference (serving)\n",
    "* Ray Datasets for scaling data loading and simple preprocessing\n",
    "* Ray Tune for scaling hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13f977-c891-4b4a-81e8-8648a7e8e9f1",
   "metadata": {},
   "source": [
    "Tune is mature, compatible with both DL frameworks PyTorch + Lightning and TensorFlow + Keras, as well as Scikit-Learn and XGBoost. It also integrates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522b8c9-fd4e-40fc-82ad-73d19016611f",
   "metadata": {},
   "source": [
    "Building a deep learning estimator requires to gradually converge to a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673e069-f12e-45c5-91c1-659175c88a6c",
   "metadata": {},
   "source": [
    "Ray produces a lot of logs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cc41e-db2a-499a-97fb-66f4bc1b48da",
   "metadata": {},
   "source": [
    "## Note on LR Scheduler\n",
    "\n",
    "LR schedulers are outside Tune's scope, those are provided by vanilla PyTorch. Learning-rate scheduling modulates the LR while in train mode, allowing DL developers to apply a cyclic LR, LR warmup, or LR decay to help escaping local minima and hopefully converge to a global maximum.\n",
    "\n",
    "The `torch.optim.lr_scheduler` package comes with a variety of schedulers, including: `ExponentialLR`, `CosineAnealingLR`, `ReduceLROnPlateau`, `CyclicLR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a024a37-5f1c-4241-aca8-1c0911d602c5",
   "metadata": {},
   "source": [
    "## Understanding the logic behind an HPO framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6b04cb-6c23-4f60-b830-d1f23184290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ray.tune as tune\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a3bbf-b457-404d-b31d-718fd2e5fc95",
   "metadata": {},
   "source": [
    "The overall is a simple 3-step process\n",
    "\n",
    "1. Sample parameters to make up an HP set, following a specific search algorithm\n",
    "2. Build an execution stack with desired number of runs, then start at the top\n",
    "3. Monitor the training on relevant metrics, stop unpromising trainings early, and move the stack with freed-up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edafc4-1c3c-42ae-94af-5efac1f873df",
   "metadata": {},
   "source": [
    "Let's first load all the necessary params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca479b7-eca0-4240-8352-cfc67db2d808",
   "metadata": {},
   "source": [
    "We will implement all of these components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cadc9b-200f-4ab3-9a35-8bbff2f165d1",
   "metadata": {},
   "source": [
    "## Exploring components in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae2212-8837-4a58-83a7-c137f4935e77",
   "metadata": {},
   "source": [
    "Ray.Tune relies on a lot fo components to achieve this:\n",
    "* Making a selection of the HParams you wish to optimize for, and setting the search space (and choosing for each parameter a sampling method.)\n",
    "* A callback to monitor and automatically report metrics progress during training\n",
    "* A trials scheduler to kill unpromising HP sets\n",
    "* A search algorithm used to explore the HP space\n",
    "* A logger to push values to a possibly remote monitor solution\n",
    "* A runner to sequentially execute experiments with the set of HP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d074c-22ad-4cca-9101-8bd43ba1639a",
   "metadata": {},
   "source": [
    "## First fully functional example with no HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015525e-f1bb-4602-961c-ee2b92b2d04c",
   "metadata": {},
   "source": [
    "### **Search space**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93208327-75ac-46ef-8ba2-ff08a073f969",
   "metadata": {},
   "source": [
    "Each HP has its own space. Ray comes standard with a range of params types. Report \n",
    "* `tune.uniform`, `tune.quniform`, and `tune.qloguniform` to uniformly sample a float in a range of values\n",
    "* `tune.randn`, `tune.qrandn`, `tune.randint`, `tune.qrandint`, `tune.lograndint`, and `tune.qlograndint` to uniformly sample an integer in a range of values\n",
    "* `tune.choice` to sample from a discrete list of values\n",
    "* `tune.sample_from` for a custom-made sampling method\n",
    "* `tune.grid_search` to end-up browsing an entire list sequentially\n",
    "\n",
    "Create a config `dict` for data and models HPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580574c0-4ec3-4875-aca5-28660df6ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"cumulate\": tune.choice([False, True]),\n",
    "        \"p\": tune.randint(2, 7)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3b7fb7-aaea-4f67-ba6f-b262462ac47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled: cumulate = False, p = 5\n",
      "sampled: cumulate = False, p = 3\n",
      "sampled: cumulate = False, p = 4\n",
      "sampled: cumulate = False, p = 2\n",
      "sampled: cumulate = False, p = 5\n",
      "sampled: cumulate = False, p = 4\n",
      "sampled: cumulate = False, p = 2\n",
      "sampled: cumulate = False, p = 2\n",
      "sampled: cumulate = True, p = 6\n",
      "sampled: cumulate = False, p = 5\n"
     ]
    }
   ],
   "source": [
    "cumulate = config[\"model\"][\"cumulate\"]\n",
    "p = config[\"model\"][\"p\"]\n",
    "\n",
    "for _ in range(10):\n",
    "    print(f\"sampled: cumulate = {cumulate.sample()}, p = {p.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac50e1a-2ca3-4ac3-9ba3-2f774940c0ab",
   "metadata": {},
   "source": [
    "### **Runner**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a85c3-66e7-4c91-8715-a2d45adb0cd0",
   "metadata": {},
   "source": [
    "The runner will execute runs of either a functionnal `trainable`, or a `tune.Trainable`, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511fc2a1-ad18-4efa-b022-a8a26f0f3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainable(tune.Trainable):\n",
    "    \n",
    "    cumulative = 0\n",
    "    \n",
    "    def setup(self, config):\n",
    "        \n",
    "        self.cumulate = config[\"cumulate\"]\n",
    "        self.p = config[\"p\"]\n",
    "\n",
    "    def step(self):\n",
    "        \n",
    "        score = 1 / self.p\n",
    "        \n",
    "        self.p += 1\n",
    "        self.cumulative += score\n",
    "        \n",
    "        time.sleep(.2)\n",
    "        if self.cumulate:\n",
    "            return {\"score\": self.cumulative}\n",
    "        \n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7114b0d-6c55-457f-b602-17fef2a1201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cumulate': <ray.tune.sample.Categorical at 0x7fff287e7700>,\n",
       " 'p': <ray.tune.sample.Integer at 0x7fff287e77c0>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = config[\"model\"]\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e05a-72d9-4d0b-9a82-1a74580e51b5",
   "metadata": {},
   "source": [
    "### **Callbacks**\n",
    "\n",
    "A callback reports values to the runner, so the scheduler can take decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1f1703-de55-4095-84f1-8885a3d7d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback(tune.Callback):\n",
    "    \n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"Current score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4c6de-ce67-4761-a546-0f478309e250",
   "metadata": {},
   "source": [
    "The next run will execute forever, because it has no stopping condition, so you'll need to manually stop it. We'll add a reason to stop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9af9cf1-a997-49cc-b4b7-08709e96c1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The configured object store size (17.138499584 GB) exceeds /dev/shm size (17.138499584 GB). This will harm performance. Consider deleting files in /dev/shm or increasing its size with --shm-size in Docker. To ignore this warning, set RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPrintCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/tune.py:321\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use custom trial executor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trial_executor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trial_executor, RayTrialExecutor):\n\u001b[0;32m--> 321\u001b[0m     \u001b[43m_ray_auto_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remote:\n\u001b[1;32m    324\u001b[0m     remote_run \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mremote(num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(run)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/tune.py:768\u001b[0m, in \u001b[0;36m_ray_auto_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    765\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing Ray automatically.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor cluster usage or custom Ray initialization, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall `ray.init(...)` before `tune.run`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 768\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:933\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m     ray_params \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mRayParams(\n\u001b[1;32m    896\u001b[0m         node_ip_address\u001b[38;5;241m=\u001b[39mnode_ip_address,\n\u001b[1;32m    897\u001b[0m         raylet_ip_address\u001b[38;5;241m=\u001b[39mraylet_ip_address,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m         metrics_export_port\u001b[38;5;241m=\u001b[39m_metrics_export_port,\n\u001b[1;32m    928\u001b[0m         tracing_startup_hook\u001b[38;5;241m=\u001b[39m_tracing_startup_hook)\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;66;03m# isn't called.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     _global_node \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshutdown_at_exit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspawn_reaper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/node.py:274\u001b[0m, in \u001b[0;36mNode.__init__\u001b[0;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gcs_client()\u001b[38;5;241m.\u001b[39minternal_kv_put(\n\u001b[1;32m    269\u001b[0m             \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracing_startup_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    270\u001b[0m             ray_params\u001b[38;5;241m.\u001b[39mtracing_startup_hook\u001b[38;5;241m.\u001b[39mencode(), \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m             ray_constants\u001b[38;5;241m.\u001b[39mKV_NAMESPACE_TRACING)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connect_only:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_ray_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# we should update the address info after the node has been started\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/node.py:1114\u001b[0m, in \u001b[0;36mNode.start_ray_processes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;66;03m# Make sure we don't call `determine_plasma_store_config` multiple\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# times to avoid printing multiple warnings.\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m resource_spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_resource_spec()\n\u001b[1;32m   1113\u001b[0m plasma_directory, object_store_memory \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 1114\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_plasma_store_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_store_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplasma_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplasma_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuge_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuge_pages\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_raylet(plasma_directory, object_store_memory)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39minclude_log_monitor:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/services.py:1899\u001b[0m, in \u001b[0;36mdetermine_plasma_store_config\u001b[0;34m(object_store_memory, plasma_directory, huge_pages)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     plasma_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dev/shm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_OBJECT_STORE_ALLOW_SLOW_STORAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1897\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m object_store_memory \u001b[38;5;241m>\u001b[39m\n\u001b[1;32m   1898\u001b[0m       ray_constants\u001b[38;5;241m.\u001b[39mREQUIRE_SHM_SIZE_THRESHOLD):\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1900\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe configured object store size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m GB) exceeds \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1901\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dev/shm size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m GB). This will harm performance. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1902\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider deleting files in /dev/shm or increasing its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1903\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1904\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--shm-size in Docker. To ignore this warning, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE=1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1906\u001b[0m             object_store_memory \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e9\u001b[39m, shm_avail \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e9\u001b[39m))\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1908\u001b[0m     plasma_directory \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_user_temp_dir()\n",
      "\u001b[0;31mValueError\u001b[0m: The configured object store size (17.138499584 GB) exceeds /dev/shm size (17.138499584 GB). This will harm performance. Consider deleting files in /dev/shm or increasing its size with --shm-size in Docker. To ignore this warning, set RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE=1."
     ]
    }
   ],
   "source": [
    "tune.run(\n",
    "    Trainable, \n",
    "    config=params, \n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    callbacks=[\n",
    "        PrintCallback()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce5a5f-3b26-4a76-a0dc-0bb541af67b1",
   "metadata": {},
   "source": [
    "### **Scheduler**\n",
    "\n",
    "Finally, the scheduler will stop the execution of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b676d1e-7076-47f6-a144-e51285f704e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = tune.schedulers.ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    max_t=100,\n",
    "    grace_period=3,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eba7f592-e4d7-4409-813d-926d8bfac4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-24 21:30:41 (running for 00:00:20.69)<br>Memory usage on this node: 8.3/94.4 GiB<br>Using AsyncHyperBand: num_stopped=10\n",
       "Bracket: Iter 81.000: -0.011811391223155929 | Iter 27.000: -0.032974910394265235 | Iter 9.000: -0.08333333333333333 | Iter 3.000: -0.4345238095238093<br>Resources requested: 0/96 CPUs, 0/0 GPUs, 0.0/55.06 GiB heap, 0.0/27.53 GiB objects<br>Current best trial: f8366_00001 with score=0.009615384615384616 and parameters={'cumulate': False, 'p': 5}<br>Result logdir: /home/jupyter/ray_results/Trainable_2022-02-24_21-30-21<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:30:42,049\tINFO tune.py:636 -- Total run time: 20.80 seconds (20.69 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    Trainable, \n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    config=params, \n",
    "    num_samples=10, \n",
    "    verbose=1,\n",
    "    scheduler=asha_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee25e9-ac2e-41c4-98b0-4178c41d30b4",
   "metadata": {},
   "source": [
    "All results are saved by Ray.Tune, you can access them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14546b84-cba5-46db-83b0-53fba911cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best config: {'cumulate': False, 'p': 5}\n",
      "best result: {'score': 0.009615384615384616, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 100, 'trial_id': 'f8366_00001', 'experiment_id': '7e374ab7198f43e9a874eb4c6a194709', 'date': '2022-02-24_21-30-41', 'timestamp': 1645738241, 'time_this_iter_s': 0.2002270221710205, 'time_total_s': 20.025014877319336, 'pid': 29881, 'hostname': 'alx', 'node_ip': '10.164.0.2', 'config': {'cumulate': False, 'p': 5}, 'time_since_restore': 20.025014877319336, 'timesteps_since_restore': 0, 'iterations_since_restore': 100, 'experiment_tag': '1_cumulate=False,p=5'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"best config: {analysis.best_config}\")\n",
    "print(f\"best result: {analysis.best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d549e-4838-428e-9f23-b672bab66475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
