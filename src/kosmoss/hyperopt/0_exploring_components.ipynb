{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Ray.io optimization framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a16b2-58ac-467e-bcb7-da78fe8b726c",
   "metadata": {},
   "source": [
    "Ray.io is a framework developed to scale compute-intensive Python workload. It relies on many components dedicated among which, notoriously:\n",
    "\n",
    "* Ray Core to scale general-purpose Python workflows\n",
    "* Ray Train for scaling DL-models training\n",
    "* Ray Serve for scaling models inference (serving)\n",
    "* Ray Datasets for scaling data loading and simple preprocessing\n",
    "* Ray Tune for scaling hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13f977-c891-4b4a-81e8-8648a7e8e9f1",
   "metadata": {},
   "source": [
    "Tune is mature, compatible with both DL frameworks PyTorch + Lightning and TensorFlow + Keras, as well as Scikit-Learn and XGBoost. It also integrates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522b8c9-fd4e-40fc-82ad-73d19016611f",
   "metadata": {},
   "source": [
    "Building a deep learning estimator requires to gradually converge to a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673e069-f12e-45c5-91c1-659175c88a6c",
   "metadata": {},
   "source": [
    "Ray produces a lot of logs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cc41e-db2a-499a-97fb-66f4bc1b48da",
   "metadata": {},
   "source": [
    "## Note on LR Scheduler\n",
    "\n",
    "LR schedulers are outside Tune's scope, those are provided by vanilla PyTorch. Learning-rate scheduling modulates the LR while in train mode, allowing DL developers to apply a cyclic LR, LR warmup, or LR decay to help escaping local minima and hopefully converge to a global maximum.\n",
    "\n",
    "These objects lie under the `torch.optim.lr_scheduler` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a024a37-5f1c-4241-aca8-1c0911d602c5",
   "metadata": {},
   "source": [
    "## Understanding the logic behind an HPO framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b6b04cb-6c23-4f60-b830-d1f23184290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray.tune as tune\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a3bbf-b457-404d-b31d-718fd2e5fc95",
   "metadata": {},
   "source": [
    "The overall is a simple 3-step process\n",
    "\n",
    "1. Sample parameters to make up an HP set, following a specific search algorithm\n",
    "2. Build an execution stack with desired number of runs, then start at the top\n",
    "3. Monitor the training on relevant metrics, stop unpromising trainings early, and move the stack with freed-up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edafc4-1c3c-42ae-94af-5efac1f873df",
   "metadata": {},
   "source": [
    "Let's first load all the necessary params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca479b7-eca0-4240-8352-cfc67db2d808",
   "metadata": {},
   "source": [
    "We will implement all of these components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cadc9b-200f-4ab3-9a35-8bbff2f165d1",
   "metadata": {},
   "source": [
    "## Exploring components in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae2212-8837-4a58-83a7-c137f4935e77",
   "metadata": {},
   "source": [
    "Ray.Tune relies on a lot fo components to achieve this:\n",
    "* Making a selection of the HParams you wish to optimize for, and setting the search space (and choosing for each parameter a sampling method.)\n",
    "* A callback to monitor and automatically report metrics progress during training\n",
    "* A trials scheduler to kill unpromising HP sets\n",
    "* A search algorithm used to explore the HP space\n",
    "* A logger to push values to a possibly remote monitor solution\n",
    "* A runner to sequentially execute experiments with the set of HP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d074c-22ad-4cca-9101-8bd43ba1639a",
   "metadata": {},
   "source": [
    "## First fully functional example with no HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015525e-f1bb-4602-961c-ee2b92b2d04c",
   "metadata": {},
   "source": [
    "### **Search space**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93208327-75ac-46ef-8ba2-ff08a073f969",
   "metadata": {},
   "source": [
    "Each HP has its own space. Ray comes standard with a range of params types. Report \n",
    "* `tune.uniform`, `tune.quniform`, and `tune.qloguniform` to uniformly sample a float in a range of values\n",
    "* `tune.randn`, `tune.qrandn`, `tune.randint`, `tune.qrandint`, `tune.lograndint`, and `tune.qlograndint` to uniformly sample an integer in a range of values\n",
    "* `tune.choice` to sample from a discrete list of values\n",
    "* `tune.sample_from` for a custom-made sampling method\n",
    "* `tune.grid_search` to end-up browsing an entire list sequentially\n",
    "\n",
    "Create a config `dict` for data and models HPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "580574c0-4ec3-4875-aca5-28660df6ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"cumulate\": tune.choice([False, True]),\n",
    "        \"p\": tune.randint(2, 7)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd3b7fb7-aaea-4f67-ba6f-b262462ac47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled: cumulate = True, p = 4\n",
      "sampled: cumulate = True, p = 2\n",
      "sampled: cumulate = False, p = 3\n",
      "sampled: cumulate = False, p = 4\n",
      "sampled: cumulate = False, p = 3\n",
      "sampled: cumulate = False, p = 2\n",
      "sampled: cumulate = False, p = 6\n",
      "sampled: cumulate = True, p = 4\n",
      "sampled: cumulate = True, p = 5\n",
      "sampled: cumulate = True, p = 5\n"
     ]
    }
   ],
   "source": [
    "cumulate = config[\"model\"][\"cumulate\"]\n",
    "p = config[\"model\"][\"p\"]\n",
    "\n",
    "for _ in range(10):\n",
    "    print(f\"sampled: cumulate = {cumulate.sample()}, p = {p.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac50e1a-2ca3-4ac3-9ba3-2f774940c0ab",
   "metadata": {},
   "source": [
    "### **Runner**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a85c3-66e7-4c91-8715-a2d45adb0cd0",
   "metadata": {},
   "source": [
    "The runner will execute runs of either a functionnal `trainable`, or a `tune.Trainable`, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "511fc2a1-ad18-4efa-b022-a8a26f0f3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainable(tune.Trainable):\n",
    "    \n",
    "    cumulative = 0\n",
    "    \n",
    "    def setup(self, config):\n",
    "        \n",
    "        self.cumulate = config[\"cumulate\"]\n",
    "        self.p = config[\"p\"]\n",
    "\n",
    "    def step(self):\n",
    "        \n",
    "        score = 1 / self.p\n",
    "        \n",
    "        self.p += 1\n",
    "        self.cumulative += score\n",
    "        \n",
    "        time.sleep(.2)\n",
    "        if self.cumulate:\n",
    "            return {\"score\": self.cumulative}\n",
    "        \n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7114b0d-6c55-457f-b602-17fef2a1201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cumulate': <ray.tune.sample.Categorical at 0x7f3c25b4aa50>,\n",
       " 'p': <ray.tune.sample.Integer at 0x7f3c25b4a250>}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = config[\"model\"]\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a18e05a-72d9-4d0b-9a82-1a74580e51b5",
   "metadata": {},
   "source": [
    "### **Callbacks**\n",
    "\n",
    "A callback reports values to the runner, so the scheduler can take decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca1f1703-de55-4095-84f1-8885a3d7d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintCallback(tune.Callback):\n",
    "    \n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"Current score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4c6de-ce67-4761-a546-0f478309e250",
   "metadata": {},
   "source": [
    "The next run will execute forever, because it has no stopping condition, so you'll need to manually stop it. We'll add a reason to stop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9af9cf1-a997-49cc-b4b7-08709e96c1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score: 0.16666666666666666\n",
      "Current score: 0.30952380952380953\n",
      "Current score: 0.43452380952380953\n",
      "Current score: 0.5456349206349207\n",
      "Current score: 0.6456349206349207\n",
      "Current score: 0.7365440115440116\n",
      "Current score: 0.819877344877345\n",
      "Current score: 0.896800421800422\n",
      "Current score: 0.9682289932289934\n",
      "Current score: 1.03489565989566\n",
      "Current score: 1.09739565989566\n",
      "Current score: 1.1562191893074247\n",
      "Current score: 1.2117747448629803\n",
      "Current score: 1.2644063238103487\n",
      "Current score: 1.3144063238103487\n",
      "Current score: 1.3620253714293964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:23:55,692\tWARNING tune.py:593 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score: 1.4074799168839418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:23:55,899\tERROR tune.py:632 -- Trials did not complete: [Trainable_10172_00000]\n",
      "2022-02-24 21:23:55,899\tWARNING tune.py:641 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f3c25b29610>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    Trainable, \n",
    "    config=params, \n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    callbacks=[\n",
    "        PrintCallback()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce5a5f-3b26-4a76-a0dc-0bb541af67b1",
   "metadata": {},
   "source": [
    "### **Scheduler**\n",
    "\n",
    "Finally, the scheduler will stop the execution of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b676d1e-7076-47f6-a144-e51285f704e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_scheduler = tune.schedulers.ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    max_t=100,\n",
    "    grace_period=3,\n",
    "    reduction_factor=3,\n",
    "    brackets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eba7f592-e4d7-4409-813d-926d8bfac4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-24 21:30:41 (running for 00:00:20.69)<br>Memory usage on this node: 8.3/94.4 GiB<br>Using AsyncHyperBand: num_stopped=10\n",
       "Bracket: Iter 81.000: -0.011811391223155929 | Iter 27.000: -0.032974910394265235 | Iter 9.000: -0.08333333333333333 | Iter 3.000: -0.4345238095238093<br>Resources requested: 0/96 CPUs, 0/0 GPUs, 0.0/55.06 GiB heap, 0.0/27.53 GiB objects<br>Current best trial: f8366_00001 with score=0.009615384615384616 and parameters={'cumulate': False, 'p': 5}<br>Result logdir: /home/jupyter/ray_results/Trainable_2022-02-24_21-30-21<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 21:30:42,049\tINFO tune.py:636 -- Total run time: 20.80 seconds (20.69 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    Trainable, \n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    config=params, \n",
    "    num_samples=10, \n",
    "    verbose=1,\n",
    "    scheduler=asha_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee25e9-ac2e-41c4-98b0-4178c41d30b4",
   "metadata": {},
   "source": [
    "All results are saved by Ray.Tune, you can access them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14546b84-cba5-46db-83b0-53fba911cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best config: {'cumulate': False, 'p': 5}\n",
      "best result: {'score': 0.009615384615384616, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 100, 'trial_id': 'f8366_00001', 'experiment_id': '7e374ab7198f43e9a874eb4c6a194709', 'date': '2022-02-24_21-30-41', 'timestamp': 1645738241, 'time_this_iter_s': 0.2002270221710205, 'time_total_s': 20.025014877319336, 'pid': 29881, 'hostname': 'alx', 'node_ip': '10.164.0.2', 'config': {'cumulate': False, 'p': 5}, 'time_since_restore': 20.025014877319336, 'timesteps_since_restore': 0, 'iterations_since_restore': 100, 'experiment_tag': '1_cumulate=False,p=5'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"best config: {analysis.best_config}\")\n",
    "print(f\"best result: {analysis.best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d549e-4838-428e-9f23-b672bab66475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
