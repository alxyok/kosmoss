{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Ray.io optimization framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f401b-88c2-44f8-9bed-f12053310d95",
   "metadata": {},
   "source": [
    "## Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bc612-e0eb-49db-9a9c-3c29aae01444",
   "metadata": {},
   "source": [
    "The simplest form of search algorithms are the **GridSearch** and **RandomSearch**. More recent research in this direction have lead to the discovery of more sophisticated algos, including **BayesOptSearch**, **OptunaSearch**, and **HEBOSearch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f75494-e2c6-4e32-8059-61fe95a483b1",
   "metadata": {},
   "source": [
    "They come as external packages of Ray\\[Tune\\], directly integrated into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac1f9a-585f-4330-8ff4-7241daaebf0e",
   "metadata": {},
   "source": [
    "## On a real-world use-case\n",
    "\n",
    "In addition to the previous components:\n",
    "* `tune.suggest` The Search Algorithm will direct the search on facts\n",
    "* `tune.reporter` The Reporter\n",
    "* `tune.logger`\n",
    "* `tune.stopper`\n",
    "\n",
    "We will apply all of the components on an example of GNN for the considered use-case.\n",
    "\n",
    "Open the file `graphs.py` and debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e639635f-1ab2-485b-ad75-c056a32eca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import os.path as osp\n",
      "from pytorch_lightning import LightningDataModule\n",
      "import torch\n",
      "from torch_geometric.data import Dataset\n",
      "from torch_geometric.loader import DataLoader\n",
      "from typing import List, Tuple\n",
      "\n",
      "from kosmoss import CONFIG, DATA_PATH, METADATA\n",
      "from kosmoss.dataproc.flows import BuildGraphsFlow\n",
      "\n",
      "\n",
      "class GNNDataset(Dataset):\n",
      "    \n",
      "    def __init__(self) -> None:\n",
      "        \n",
      "        self.timestep = str(CONFIG['timestep'])\n",
      "        self.params = METADATA[str(self.timestep)]['features']\n",
      "        self.num_shards = self.params['num_shards']\n",
      "        \n",
      "        super().__init__(DATA_PATH)\n",
      "\n",
      "    @property\n",
      "    def raw_file_names(self) -> list:\n",
      "        return [\"\"]\n",
      "\n",
      "    @property\n",
      "    def processed_file_names(self) -> List[str]:\n",
      "        return [osp.join(f\"graphs-{self.timestep}\", f\"data-{shard}.pt\") \n",
      "                for shard in np.arange(self.num_shards)]\n",
      "    \n",
      "    \n",
      "    def download(self) -> None:\n",
      "        raise Exception(\"Execute the Notebooks in this Bootcamp following the order defined by the Readme.\")\n",
      "\n",
      "        \n",
      "    def process(self) -> None:\n",
      "        BuildGraphsFlow()\n",
      "        \n",
      "    def len(self) -> int:\n",
      "        return self.params['dataset_len']\n",
      "\n",
      "    def get(self, idx: int) -> Tuple[torch.Tensor]:\n",
      "        \n",
      "        shard_size = self.len() // self.num_shards\n",
      "        fileidx = idx // shard_size\n",
      "        rowidx = idx % shard_size\n",
      "        \n",
      "        data_list = torch.load(osp.join(self.processed_dir, f\"graphs-{self.timestep}\", f'data-{fileidx}.pt'))\n",
      "        data = data_list[rowidx]\n",
      "        \n",
      "        return data\n",
      "\n",
      "\n",
      "class LitGNNDataModule(LightningDataModule):\n",
      "    \n",
      "    def __init__(self, batch_size: int) -> None:\n",
      "        self.bs = batch_size\n",
      "        super().__init__()\n",
      "        \n",
      "    def prepare_data(self) -> None:\n",
      "        pass\n",
      "    \n",
      "    def setup(self, stage: str) -> None:\n",
      "        dataset = GNNDataset().shuffle()\n",
      "        length = len(dataset)\n",
      "        \n",
      "        self.testds = dataset[int(length * .9):]\n",
      "        self.valds = dataset[int(length * .8):int(length * .9)]\n",
      "        self.trainds = dataset[:int(length * .8)]\n",
      "    \n",
      "    \n",
      "    def train_dataloader(self) -> DataLoader:\n",
      "        return DataLoader(self.trainds, batch_size=self.bs, num_workers=4, shuffle=True)\n",
      "    \n",
      "    def val_dataloader(self) -> DataLoader:\n",
      "        return DataLoader(self.valds, batch_size=self.bs, num_workers=4)\n",
      "        \n",
      "    def test_dataloader(self) -> DataLoader:\n",
      "        return DataLoader(self.testds, batch_size=self.bs, num_workers=4)\n"
     ]
    }
   ],
   "source": [
    "!cat graphs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d58bfe-f2d2-48fe-bebc-06d4372054e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pytorch_lightning import LightningModule\n",
      "import torch\n",
      "from torch_geometric.data import Batch\n",
      "from torch_geometric.nn.models import GAT\n",
      "import torch_optimizer as optim\n",
      "import torchmetrics.functional as F\n",
      "from typing import Dict, List, Union\n",
      "\n",
      "\n",
      "class LitGAT(LightningModule):\n",
      "    \n",
      "    def __init__(self, **kwargs) -> None:\n",
      "        super().__init__()\n",
      "        self.save_hyperparameters()\n",
      "        self.lr = kwargs.pop('lr')\n",
      "        self.net = GAT(**kwargs)\n",
      "\n",
      "    def forward(self, \n",
      "                x: torch.Tensor, \n",
      "                edge_index: torch.Tensor) -> torch.Tensor:\n",
      "        \n",
      "        return self.net(x, edge_index)\n",
      "    \n",
      "    def _common_step(self, \n",
      "                     batch: Batch, \n",
      "                     batch_idx: int, \n",
      "                     stage: Union['train', 'val', 'test'] = \"train\") -> List[torch.Tensor]:\n",
      "        \n",
      "        y_hat = self(batch.x, batch.edge_index)\n",
      "        loss = F.mean_squared_error(y_hat, batch.y)\n",
      "        self.log(f\"{stage}_loss\", loss, prog_bar=True, on_step=True)\n",
      "        return y_hat, loss\n",
      "\n",
      "    def training_step(self, batch: Batch, batch_idx: int) -> torch.Tensor:\n",
      "        _, loss = self._common_step(batch, batch_idx, \"train\")\n",
      "        return loss\n",
      "\n",
      "    def validation_step(self, batch: Batch, batch_idx: int) -> Dict[str, torch.Tensor]:\n",
      "        _, _ = self._common_step(batch, batch_idx, \"val\")\n",
      "        # return {'val_loss': loss}\n",
      "\n",
      "    def test_step(self, batch: Batch, batch_idx: int) -> None:\n",
      "        _, _ = self._common_step(batch, batch_idx, \"test\")\n",
      "        \n",
      "    def configure_optimizers(self) -> optim.Optimizer:\n",
      "        return optim.AdamP(self.parameters(), lr=self.lr)"
     ]
    }
   ],
   "source": [
    "!cat graphnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c60c2f1-67e1-4eef-b66b-60e8cd81ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import numpy as np\n",
      "import os\n",
      "import os.path as osp\n",
      "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
      "from pytorch_lightning.accelerators import Accelerator\n",
      "from pytorch_lightning.callbacks.base import Callback\n",
      "from pytorch_lightning.loggers.wandb import WandbLogger\n",
      "from ray import tune\n",
      "from ray.tune import CLIReporter\n",
      "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
      "from ray.tune.integration.wandb import WandbLoggerCallback\n",
      "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
      "from ray.tune.suggest.hebo import HEBOSearch\n",
      "import torch\n",
      "from typing import List, Union\n",
      "\n",
      "from kosmoss import ARTIFACTS_PATH, LOGS_PATH\n",
      "from kosmoss.hyperopt.data import LitGNNDataModule\n",
      "from kosmoss.hyperopt.models import LitGAT\n",
      "\n",
      "\n",
      "def main() -> None:\n",
      "    \n",
      "    project = os.environ['wandb_project']\n",
      "    \n",
      "    def train_gnns(config: dict,\n",
      "                   num_epochs: int = 10,\n",
      "                   num_gpus: int = 0) -> None:\n",
      "        \n",
      "        datamodule = LitGNNDataModule(**config['data'])\n",
      "        model = LitGAT(**config[\"model\"])\n",
      "    \n",
      "        kwargs = {\n",
      "            \"gpus\": num_gpus,\n",
      "            \"strategy\": \"ddp\",\n",
      "            \"max_epochs\": num_epochs,\n",
      "            \"callbacks\": [\n",
      "                TuneReportCallback({\n",
      "                    \"train_loss\": \"train_loss\",\n",
      "                    \"val_loss\": \"val_loss\"\n",
      "                }, on=[\n",
      "                    \"batch_end\",\n",
      "                    \"validation_end\"\n",
      "                ]),\n",
      "            ],\n",
      "            \"progress_bar_refresh_rate\": 0\n",
      "        }\n",
      "        trainer = Trainer(**kwargs)\n",
      "        trainer.fit(model, datamodule)\n",
      "    \n",
      "    def tune_gnns_asha(num_samples: int = 4, \n",
      "                       num_epochs: int = 10, \n",
      "                       gpus_per_trial: int = 0) -> dict:\n",
      "    \n",
      "        config = {\n",
      "            \"data\": {\"batch_size\": 256},\n",
      "            \"model\": {\n",
      "                # Fixed ins-and-outs\n",
      "                \"in_channels\": 20,\n",
      "                \"out_channels\": 4,\n",
      "                \n",
      "                # HPs, each sampled from its own search space\n",
      "                \"hidden_channels\": int(tune.choice([2 ** k for k in np.arange(4, 6)]).sample()),\n",
      "                \"edge_dim\": tune.choice([2 ** k for k in np.arange(4, 6)]),\n",
      "                \"num_layers\": tune.randint(4, 10),\n",
      "                \"lr\": tune.loguniform(1e-4, 1e-1),\n",
      "                \"dropout\": tune.uniform(0, 1),\n",
      "                \"heads\": int(tune.choice([4, 8]).sample())\n",
      "            }\n",
      "        }\n",
      "\n",
      "        scheduler = ASHAScheduler(\n",
      "            max_t=num_epochs,\n",
      "            grace_period=num_epochs,\n",
      "            reduction_factor=2)\n",
      "\n",
      "        # Add on-the-fly named parameters to the set, as extra\n",
      "        train_with_params_fn = tune.with_parameters(\n",
      "            train_gnns,\n",
      "            num_epochs=num_epochs,\n",
      "            num_gpus=gpus_per_trial)\n",
      "\n",
      "        # Run the execution flow\n",
      "        analysis = tune.run(\n",
      "            # Set the callable an resources to mobilize for every trial\n",
      "            run_or_experiment=train_with_params_fn,\n",
      "            resources_per_trial={\n",
      "                \"cpu\": 4, \n",
      "                \"gpu\": gpus_per_trial\n",
      "            },\n",
      "            \n",
      "            # Set the metric to watch, and how to consider metric progress\n",
      "            metric=\"val_loss\",\n",
      "            mode=\"min\",\n",
      "            \n",
      "            # Set the config dict of all HPs\n",
      "            config=config,\n",
      "            \n",
      "            # Set the total number of tries\n",
      "            num_samples=num_samples,\n",
      "            \n",
      "            # Set the execution scheduler\n",
      "            scheduler=scheduler,\n",
      "            \n",
      "            # Set the search algorithm for sampling HPs\n",
      "            search_alg=HEBOSearch(), #metric=\"val_loss\", mode=\"min\"),\n",
      "            \n",
      "            # Give a name prefix to all experiments\n",
      "            name=\"tune_gnns_asha\",\n",
      "            \n",
      "            # Set the logger to push results to your favorite logger, and local log dir\n",
      "            callbacks=[WandbLoggerCallback(project=project, dir=LOGS_PATH)],\n",
      "            local_dir=LOGS_PATH,\n",
      "            # max_concurrent_trials=None\n",
      "            \n",
      "            # And Sh**-**!\n",
      "            verbose=0\n",
      "        )\n",
      "        \n",
      "        return analysis.best_config\n",
      "    \n",
      "    best_config = tune_gnns_asha()\n",
      "    print(\"Best HP-set yet: \", best_config)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    # Turn on this flag to allow the report callback to fail gracefully if all metrics are not populated at each 'on' step\n",
      "    os.environ[\"TUNE_DISABLE_STRICT_METRIC_CHECKING\"] = 1\n",
      "    main()"
     ]
    }
   ],
   "source": [
    "!cat runtune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2ba8f-c4fa-4d95-adfe-280e0f091be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c1d49-1b94-4618-8a68-23dba8a5894b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa86f5-aefd-4f32-a113-f6e8b89da7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
