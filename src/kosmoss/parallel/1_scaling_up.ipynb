{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# 1. Scaling UP (mono-node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf5bcd-16b3-4425-8c1b-d47ef2fc3f23",
   "metadata": {},
   "source": [
    "A Note on GPU training. We naturally assume that GPU is better than CPU, but it really depends on the workflow. You need to saturate the GPU memory, and compute surface.\n",
    "\n",
    "This session is focused on providing the candidates with minimum information to scale their current workflow with HW acceleration **AT THE APPLICATION LEVEL**. Low-level, high-value optimization is also a viable angle to address distributed training and inference, but this session does not cover it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b21f73-cca9-4061-aaee-6014690fb77f",
   "metadata": {},
   "source": [
    "## What's with GPUs anyway?\n",
    "\n",
    "DL is basically linear algebra, with a few non-linear Maths. It turns out, GPUs are a great tool to process that kind of computations. A few pieces of information\n",
    "\n",
    "In 2022, Nvidia is the leader in HW dedicated to DL. The company was the first to develop and push a [suite of libraries based on CUDA](https://developer.nvidia.com/gpu-accelerated-libraries) called CUDA-X for HW acceleration of ML/DL workloads, among which:\n",
    "* `cuBLAS`, `cuFFT`, `CUDA MathLib`, `cuRAND`, `cuSOLVER`, `cuSPARSE`, `cuTENSOR` for GPU-accelerated basic linear algebra (2D + nD), Fast Fourier Transform, and standard Math primitives, computations on sparse matrices\n",
    "* `cuDNN` for GPU-accelerated primitives for Deep NN\n",
    "* `TensorRT` for high-performance DL inference optimizer and runtime for production deployment\n",
    "* `DALI`, a portable open-source format for decoding and agumenting images and videos\n",
    "* Additionally, Nvidia GPUs rely on the NCCL library for fast, multi-GPU, multi-node communications, also a great tool for distributed DL.\n",
    "\n",
    "AMD also has a less-mature ML support with the [ROCm framework](https://www.amd.com/en/graphics/servers-solutions-rocm-ml).\n",
    "\n",
    "A few startups have started to tackle the HW problem on very different angles, notably:\n",
    "* [Graphcore](https://www.graphcore.ai/products/ipu) with its IPU die—250 TFlop and high in-processor-memory—, and SW stack (Poplar SDK) to convert existing TF and PT models into IPU-executable code\n",
    "* [Cerebras](https://cerebras.net/chip/) with its massive 850,000 cores chip—the Wafer-Scale Engine—and high-bandwidth memory and memory-per-core\n",
    "\n",
    "Google has also invested in Tensor-optimized HW with its [TPU devices](https://cloud.google.com/tpu) now only available in its [cloud platform GCP](https://cloud.google.com/) since version 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fc516-0a24-450c-bd45-0840f9a8b9b8",
   "metadata": {},
   "source": [
    "## A note on cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181424c-68e4-4a06-9361-2464b51f0525",
   "metadata": {},
   "source": [
    "You might need to clean up your ghost runs if something fails and break the training logic. You can do this one of two ways:\n",
    "* If run inside the same PID as the training from a `python train.py`:\n",
    "<code>\n",
    "import gc, torch; gc.collect(); torch.cuda.empty_cache()\n",
    "</code>\n",
    "* Otherwise, try to kill the job still running on the GPU, by get the ghost job's PID with the command `nvitop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a434084d-9827-458a-8835-3281f4f04803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 09 10:01:36 2022\n",
      "╒═════════════════════════════════════════════════════════════════════════════╕\n",
      "│ NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     │\n",
      "├───────────────────────────────┬──────────────────────┬──────────────────────┤\n",
      "│ GPU  Name        Persistence-M│ Bus-Id        Disp.A │ Volatile Uncorr. ECC │\n",
      "│ Fan  Temp  Perf  Pwr:Usage/Cap│         Memory-Usage │ GPU-Util  Compute M. │\n",
      "╞═══════════════════════════════╪══════════════════════╪══════════════════════╡\n",
      "│\u001b[32m   0  A100-SXM4-40GB      Off  \u001b[0m│\u001b[32m 00000000:07:00.0 Off \u001b[0m│\u001b[32m                    0 \u001b[0m│\n",
      "│\u001b[32m MAX   29C    P0    53W / 400W \u001b[0m│\u001b[32m      3MiB / 39.59GiB \u001b[0m│\u001b[32m      0%      Default \u001b[0m│\n",
      "├───────────────────────────────┼──────────────────────┼──────────────────────┤\n",
      "│\u001b[32m   1  A100-SXM4-40GB      Off  \u001b[0m│\u001b[32m 00000000:0F:00.0 Off \u001b[0m│\u001b[32m                    0 \u001b[0m│\n",
      "│\u001b[32m MAX   28C    P0    53W / 400W \u001b[0m│\u001b[32m      3MiB / 39.59GiB \u001b[0m│\u001b[32m      0%      Default \u001b[0m│\n",
      "╘═══════════════════════════════╧══════════════════════╧══════════════════════╛\n",
      "\u001b[1m\u001b[36m[ CPU: ▏ 0.4%                             ]\u001b[0m  \u001b[1m( Load Average:  4.92 24.88 35.85 )\u001b[0m\n",
      "\u001b[1m\u001b[35m[ MEM: █▎ 4.0%                            ]\u001b[0m  \u001b[1m\u001b[34m[ SWP: ▏ 0.0%                     ]\u001b[0m\n",
      "\n",
      "╒══════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Processes:                                                 \u001b[1m\u001b[33mroot\u001b[0m\u001b[1m@\u001b[0m\u001b[1m\u001b[32m9f0287a6b2a6\u001b[0m │\n",
      "│ GPU     PID      USER  GPU-MEM %SM  %CPU  %MEM  TIME  COMMAND                │\n",
      "╞══════════════════════════════════════════════════════════════════════════════╡\n",
      "│  No running processes found                                                  │\n",
      "╘══════════════════════════════════════════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "!nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c8c407-77c8-4996-8b61-02931a536e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: (99999999): No such process\n"
     ]
    }
   ],
   "source": [
    "# Replace in the command below the PID=99999999 by the PID number produced by nvitop\n",
    "!sudo kill -15 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0b0bb-8d00-4755-9627-35400021164e",
   "metadata": {},
   "source": [
    "## 1.1. Achieving Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "Now let's go single-node multi-GPU. The same model will be pushed to all available devices, each of which will\n",
    "1. Perform forward pass with its specific batch of data\n",
    "2. Compute the loss and perform backward pass including weights update\n",
    "4. The weights are then collected are synchronized across all devices for next pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcddaec-41af-48e4-abc1-049ddd51771d",
   "metadata": {},
   "source": [
    "#### **1.1.1. Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b1f56-ceba-420d-b0c6-a15405b7ff7b",
   "metadata": {},
   "source": [
    "DP consists of parallelizing the model, and training each instance of the model with a different mini-batch of data of size `batch_size // num_parallel_instances`. Each model will converge differently on its mini-batch, so the weights are collected and usually averaged after `p` batches, then synchronized with all instances for the next round of passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4c97-7b50-4a5c-990e-209cc0db5635",
   "metadata": {},
   "source": [
    "#### **0. Over CPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdce8d-9047-4185-b688-5af9330b671f",
   "metadata": {},
   "source": [
    "Let's launch a reference training on CPU. Take a look at the `trainup_cpu.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81332c0-32d3-47f8-9912-dc4db5207038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name                | Type       | Params\n",
      "---------------------------------------------------\n",
      "0 | normalization_layer | Normalize  | 0     \n",
      "1 | net                 | Sequential | 488 K \n",
      "---------------------------------------------------\n",
      "488 K     Trainable params\n",
      "0         Non-trainable params\n",
      "488 K     Total params\n",
      "1.955     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: num_workers>0, persistent_workers=False, and strategy=ddp_spawn may result in data loading bottlenecks. Consider setting persistent_workers=True (this is a limitation of Python .spawn() and PyTorch)\n",
      "  rank_zero_warn(\n",
      "[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/spawn.py\", line 125, in _main\n",
      "    prepare(preparation_data)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "  File \"/usr/lib/python3.8/multiprocessing/spawn.py\", line 236, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/usr/lib/python3.8/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n",
      "    main_content = runpy.run_path(main_path,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 265, in run_path\n",
      "    return _run_module_code(code, init_globals, run_name,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 97, in _run_module_code\n",
      "    _run_code(code, mod_globals, init_globals,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/kosmoss/src/kosmoss/parallel/trainup_cpu.py\", line 9, in <module>\n",
      "    from kosmoss.parallel.mlp import LitMLP\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kosmoss/parallel/mlp.py\", line 5, in <module>\n",
      "    import torch_optimizer as optim\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_optimizer/__init__.py\", line 38, in <module>\n",
      "    from .pid import PID\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 657, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 562, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap>\", line 541, in _init_module_attrs\n",
      "  File \"<frozen importlib._bootstrap>\", line 382, in cached\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 487, in _get_cached\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 354, in cache_from_source\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffe8b54b040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1295, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    }
   ],
   "source": [
    "!python trainup_cpu.py --batch-size 512 \\\n",
    "                       --num-processes 2 > ${HOME}/.kosmoss/logs/trainup_cpu.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18532a-4a29-4ffd-9b06-c60f636cc627",
   "metadata": {},
   "source": [
    "#### **1. Launching a training on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52ae8c9-f0ec-4103-b18d-1c86951af0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import psutil\n",
      "from pytorch_lightning import Trainer, seed_everything\n",
      "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "from typing import Union\n",
      "\n",
      "from kosmoss import CONFIG, LOGS_PATH, METADATA\n",
      "from kosmoss.parallel.data import FlattenedDataModule\n",
      "from kosmoss.parallel.mlp import LitMLP\n",
      "\n",
      "def main(batch_size: int,\n",
      "         lr: float,\n",
      "         strategy: Union['ddp', 'horovod'],\n",
      "         gpus: int,\n",
      "         num_nodes: int) -> None:\n",
      "\n",
      "    seed_everything(42, workers=True)\n",
      "    \n",
      "    step = CONFIG['timestep']\n",
      "    params = METADATA[str(step)]['flattened']\n",
      "\n",
      "    x_feats = params['x_shape'][-1]\n",
      "    y_feats = params['y_shape'][-1]\n",
      "\n",
      "    mlp = LitMLP(\n",
      "        in_channels=x_feats,\n",
      "        hidden_channels=100,\n",
      "        out_channels=y_feats,\n",
      "        \n",
      "        # Adjust the learning rate accordingly to account for the increase in total batch size\n",
      "        # Or use a Lightning LR Finder functionality, or any other framework's finder\n",
      "        lr=lr,\n",
      "    )\n",
      "\n",
      "    cores = psutil.cpu_count(logical=False)\n",
      "    datamodule = FlattenedDataModule(\n",
      "        \n",
      "        # Adjust the total batch size with regards to the number of nodes and GPUs per node\n",
      "        batch_size=batch_size // (num_nodes * gpus),\n",
      "        num_workers=cores\n",
      "    )\n",
      "\n",
      "    logger = TensorBoardLogger(\n",
      "        save_dir=LOGS_PATH,\n",
      "        name='flattened_mlp_logs',\n",
      "        log_graph=True\n",
      "    )\n",
      "\n",
      "    gpu_trainer = Trainer(\n",
      "        strategy=strategy,\n",
      "        gpus=gpus,\n",
      "        \n",
      "        # If you're using a batch normalization layer\n",
      "        # The following flag can allow sync accros total batch instead of local minibatch\n",
      "        # sync_batchnorm=True,\n",
      "        \n",
      "        max_epochs=1,\n",
      "        logger=logger,\n",
      "        deterministic=True,\n",
      "        num_sanity_val_steps=0,\n",
      "        num_nodes=num_nodes\n",
      "        \n",
      "        # Uncomment if you want to use Nvidia's own implementation of AMP called APEX\n",
      "        # amp_backend='apex',\n",
      "        \n",
      "        # Useful if you want to profile your training for debug purposes\n",
      "        # profiler=\"simple\"\n",
      "    )\n",
      "    gpu_trainer.fit(model=mlp, datamodule=datamodule)\n",
      "    gpu_trainer.test(model=mlp, datamodule=datamodule)\n",
      "\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    import argparse\n",
      "    \n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--batch-size', \n",
      "                        type=int,\n",
      "                        help='Total batch size', \n",
      "                        default=16)\n",
      "    parser.add_argument('--lr', \n",
      "                        type=float,\n",
      "                        help='Learning Rate', \n",
      "                        default=1e-4)\n",
      "    parser.add_argument('--strategy', \n",
      "                        type=str,\n",
      "                        help='Data Distributed Strategy', \n",
      "                        default='ddp')\n",
      "    parser.add_argument('--gpus', \n",
      "                        type=int,\n",
      "                        help='Number of GPUs to accelerate over', \n",
      "                        default=1)\n",
      "    parser.add_argument('--num-nodes', \n",
      "                        type=int,\n",
      "                        help='Number of nodes to accelerate over', \n",
      "                        default=1)\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    main(\n",
      "        args.batch_size, \n",
      "        args.lr,\n",
      "        args.strategy, \n",
      "        args.gpus,\n",
      "        args.num_nodes\n",
      "    )"
     ]
    }
   ],
   "source": [
    "!cat trainup_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0164d-921b-450a-886e-24e0031dd942",
   "metadata": {},
   "source": [
    "Let's launch the training with 2 nodes and 1 GPU/node. Since we're on a single node, each node designates an independent process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c9f1fd8-16f2-4e20-b97d-9b5b6d078338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Global seed set to 42\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=2764243): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python trainup_gpu.py --batch-size 512 \\\n",
    "                      --lr=1e-4 \\\n",
    "                      --strategy 'ddp' \\\n",
    "                      --gpus 1 \\\n",
    "                      --num-nodes 2 > ${HOME}/.kosmoss/logs/trainup_gpu_ddp.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67b3c6-ebdb-4780-be9d-ab6a8768eabe",
   "metadata": {},
   "source": [
    "## **1.2. A Note on Model Parallelism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280a2db-6f67-41b7-85b6-d4a44af262de",
   "metadata": {},
   "source": [
    "You should really go for model parallelism starting at 500M parameters. \n",
    "\n",
    "No material on that since the subject is complex and would require an entire session, just know that it exists.\n",
    "\n",
    "Large topic, look at the [overview from Lightning guides](https://pytorch-lightning.readthedocs.io/en/stable/advanced/advanced_gpu.html#choosing-an-advanced-distributed-gpu-plugin), or the [in-depth documentation for the FairScale initiative](https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1df06b-1cd5-4f84-8bbf-03eaa6fc4937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
