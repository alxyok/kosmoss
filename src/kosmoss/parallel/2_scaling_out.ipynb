{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Scaling out (multi-nodes + multi-GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "*Lightning integrates with horovod, standard.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11033d9a-6f48-42e0-8e8b-b966d4ad696f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Horovod\n",
    "\n",
    "Horovods is an [Open Source framework](https://horovod.readthedocs.io/en/stable/summary_include.html) backed by Uber for distributed DL, compatible with TensorFlow, PyTorch and MXNet. \n",
    "\n",
    "Parallelism is done by [SPMD programming](https://en.wikipedia.org/wiki/SPMD) with MPI, and its development was motivated by the following leitmotiv: 'Internally at Uber we found the MPI model to be much more straightforward and require far less code changes than previous solutions such as Distributed TensorFlow with parameter servers. Once a training script has been written for scale with Horovod, it can run on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes'.\n",
    "\n",
    "Horovod encompasses the design principle for any core MPI program. [More info](https://horovod.readthedocs.io/en/stable/concepts.html) \n",
    "* Size: number of processes\n",
    "* Rank: unique process identifier\n",
    "* Lank rank: unique process identifier within the server\n",
    "* AllReduce: operation that aggregates data among multiple processes and distributes them back to them\n",
    "* AllGather: operation that gathers data from all processes on every process\n",
    "* Broadcast: operation that broadcasts data from one to every other processes\n",
    "* AllToAll: operation to distribute data between all processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc91e8-6123-4315-8cfa-ed00a2492108",
   "metadata": {},
   "source": [
    "Adapting an existing code from pure TensorFlow + Keras or pure PyTorch to Horovod [is just a few lines of code](https://horovod.readthedocs.io/en/stable/pytorch.html).\n",
    "\n",
    "Hopefully, Lightning takes care of the burden for us. Simply change the `accelerator` option of the `pl.Trainer` to `horovod`, then run the command below for single-node execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682cad8-85fd-421d-88b7-046c9a4848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "horovodrun -np 4 python trainout.py > ${HOME}/.kosmoss/logs/trainout_gpu_horovod.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd6a7b-5e18-4b03-82b8-98686ee2584d",
   "metadata": {},
   "source": [
    "Contraire to native DistributedDataParallel detailed in the previous section, no need to adjust the learning rate `lr` this time, horovod takes care of that underneath.\n",
    "\n",
    "To go multi-node, use the same mpirun execution syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04496d-7949-440d-95a9-c3f99f54740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "horovodrun -np 8 -H hostname1:4,hostname2:4 python trainout.py > ${HOME}/.kosmoss/logs/trainout_gpu_horovod.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb90766-1da8-43da-b7af-27e9df388981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
