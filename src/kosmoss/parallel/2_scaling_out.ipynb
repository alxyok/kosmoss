{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Scaling out (multi-nodes + multi-GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "Lightning integrates, standardly, a lot of options to go for multi-nodes, multi-GPUs training on a cluster. For HPC-grade cluster, it would be fooling not to leverage Interconnect capabilities. Lightning makes it easy to launch with SLURM at negligeable cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11033d9a-6f48-42e0-8e8b-b966d4ad696f",
   "metadata": {},
   "source": [
    "### SLURM\n",
    "\n",
    "SLURM is an Open Source, flexible job scheduler used to manage resources in an HPC context, in 3 keys functions:\n",
    "* Exclusive or non-exclusive resource allocation system for a specific time period\n",
    "* A tool for executing and monitoring jobs on a set of allocated resources\n",
    "* A scheduling system that manages contention for resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3127a0d-3ad4-4c4d-81df-4f3a545f7fe9",
   "metadata": {},
   "source": [
    "### Quick-guide to Resources Allocation and Job Submission\n",
    "\n",
    "**This part is just a brief introduction to SLURM not intended as a comprehensive review.**\n",
    "\n",
    "A few commands:\n",
    "\n",
    "* Listing visible cluster resources with `sinfo <options>`\n",
    "* Allocate resources with `salloc <resources_type_and_options>`\n",
    "* Run commands directly on allocated resources with an `srun <command>`\n",
    "* Run an ensemble of commands with a call to `sbatch <script>`\n",
    "\n",
    "Running command ends-up in SLURM building execution environment, including setting variables and network devices so allocated nodes can communicate with Interconnect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e08e6-807d-4d58-9347-bb20bcc168c0",
   "metadata": {},
   "source": [
    "### Scale-out with SLURM\n",
    "\n",
    "The `trainout.py` Python script is very similar to the regular scale-up method developed in `trainup_gpu.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03f5677-36c0-42d3-907a-38745beadb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pytorch_lightning import Trainer\n",
      "from pytorch_lightning.plugins.environments import SLURMEnvironment\n",
      "\n",
      "from kosmoss.parallel.data import FlattenedDataModule\n",
      "from kosmoss.parallel.models import LitMLP\n",
      "\n",
      "# This file is for launching a training with an srun. To perform a run with a SlurmCluster object, follow the guide at https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html#building-slurm-scripts. We are not however much interested in the grid search promoted by the guide since we will be using the Ray Tune framwork in the last part.\n",
      "\n",
      "def main(config):\n",
      "    \n",
      "    datamodule = FlattenedDataModule(**config[\"data\"])\n",
      "    model = LitMLP(**config[\"models\"])\n",
      "    \n",
      "    trainer = Trainer(\n",
      "        gpus=8, \n",
      "        num_nodes=4, \n",
      "        strategy=\"ddp\",\n",
      "        \n",
      "        # By default, a failed sbatch will be resubmitted\n",
      "        # To deactivate the behavior, configure the SLURM environment with no auto_requeue\n",
      "        plugins=[SLURMEnvironment(auto_requeue=False)]\n",
      "    )\n",
      "    trainer.fit(model, datamodule=datamodule)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    \n",
      "    config = {\n",
      "        \"data\": {\n",
      "            \"batch_size\": 256,\n",
      "            \"num_workers\": 16,\n",
      "        },\n",
      "        \"models\": {\n",
      "            \"in_channels\": 20,\n",
      "            \"hidden_channels\": 100,\n",
      "            \"out_channels\": 4,\n",
      "            \"lr\": 1e-4\n",
      "        }\n",
      "    }\n",
      "\n",
      "    main(config)"
     ]
    }
   ],
   "source": [
    "!cat trainout.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd0f48-795c-4679-811b-c69d6675377e",
   "metadata": {},
   "source": [
    "In our case, the resource allocation has been configured within the sbatch script `submit.sbatch` itself—lines 3 to 8 serve this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e75902-4b90-427f-b110-9cbe5211d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash -l\n",
      "\n",
      "#SBATCH --nodes=2\n",
      "#SBATCH --gres=gpu:2\n",
      "#SBATCH --ntasks-per-node=8\n",
      "#SBATCH --mem=0\n",
      "#SBATCH --time=0-00:30:00\n",
      "#SBATCH -p dgx_a100\n",
      "#SBATCH --signal=SIGUSR1@90\n",
      "\n",
      "# activate conda env\n",
      "source activate $1\n",
      "\n",
      "# debugging flags (optional)\n",
      "export NCCL_DEBUG=INFO\n",
      "export PYTHONFAULTHANDLER=1\n",
      "\n",
      "# on your cluster you might need these:\n",
      "# set the network interface\n",
      "# export NCCL_SOCKET_IFNAME=^docker0,lo\n",
      "\n",
      "# might need the latest CUDA\n",
      "# module load NCCL/2.4.7-1-cuda.10.0\n",
      "\n",
      "# select the torch distributed backend. 'nccl' or 'gloo'\n",
      "export PL_TORCH_DISTRIBUTED_BACKEND='nccl'\n",
      "\n",
      "# run script from above\n",
      "srun python trainout.py"
     ]
    }
   ],
   "source": [
    "!cat submit.sbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda18df-ef75-4080-80ad-c2b5e996c28c",
   "metadata": {},
   "source": [
    "In details:\n",
    "* `SBATCH --nodes=2` requests 2 Nodes\n",
    "* `SBATCH --gres=gpu:2` requests 2 GPUs\n",
    "* `SBATCH --ntasks-per-node=8` requests 8 tasks per Node\n",
    "* `SBATCH --mem=0` requests all memory available on the the Node\n",
    "* `SBATCH --time=0-00:30:00` set the limit on total run time to 30min\n",
    "* `SBATCH -p dgx_a100` select only Nodes from the dgx_a100 partition\n",
    "* `SBATCH --signal=SIGUSR1@90` signal the job with SIGUSR1 when it's 90 seconds to its ending time, so that it can save the model weights on disk—this is not automatic and needs to be implemented on signal receive\n",
    "\n",
    "Finally, you can load the environment with specific packages with the command `module load <library>`, and launch the training in an `srun python trainout.py`.\n",
    "\n",
    "Run the `sbatch submit.sbatch` command the execute the script on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb90766-1da8-43da-b7af-27e9df388981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
