{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523e7e12-dac9-47c4-8d67-a210ff80daea",
   "metadata": {},
   "source": [
    "# Scaling out (multi-nodes + multi-GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebfdb2-ed51-4a06-9230-9d44af08bc7c",
   "metadata": {},
   "source": [
    "Lightning integrates, standardly, a lot of options to go for multi-nodes, multi-GPUs training on a cluster. For HPC-grade cluster, it would be fooling not to leverage Interconnect capabilities. Lightning makes it easy to launch with SLURM at negligeable cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11033d9a-6f48-42e0-8e8b-b966d4ad696f",
   "metadata": {},
   "source": [
    "### SLURM\n",
    "\n",
    "SLURM is an Open Source, flexible job scheduler used to manage resources in an HPC context, in 3 keys functions:\n",
    "* Exclusive or non-exclusive resource allocation system for a specific time period\n",
    "* A tool for executing and monitoring jobs on a set of allocated resources\n",
    "* A scheduling system that manages contention for resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3127a0d-3ad4-4c4d-81df-4f3a545f7fe9",
   "metadata": {},
   "source": [
    "### Quick-guide to Resources Allocation and Job Submission\n",
    "\n",
    "**This part is just a brief introduction to SLURM not intended as a comprehensive review.**\n",
    "\n",
    "A few commands:\n",
    "\n",
    "* Listing visible cluster resources with `sinfo <options>`\n",
    "* Allocate resources with `salloc <resources_type_and_options>`\n",
    "* Run commands directly on allocated resources with an `srun <command>`\n",
    "* Run an ensemble of commands with a call to `sbatch <script>`\n",
    "\n",
    "Running command ends-up in SLURM building execution environment, including setting variables and network devices so allocated nodes can communicate with Interconnect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e08e6-807d-4d58-9347-bb20bcc168c0",
   "metadata": {},
   "source": [
    "### Scale-out with SLURM\n",
    "\n",
    "The `trainout.py` Python script is very similar to the regular scale-up method developed in `trainup_gpu.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03f5677-36c0-42d3-907a-38745beadb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import os.path as osp\n",
      "from pytorch_lightning import Trainer\n",
      "\n",
      "from kosmoss.parallel.models import LitMLP\n",
      "\n",
      "# This file is for launching a training with an srun. To perform a run with a SlurmCluster object, follow the guide at https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html#building-slurm-scripts. We are not however much interested in the grid search promoted by the guide since we will be using the Ray Tune framwork in the last part.\n",
      "\n",
      "def main(config):\n",
      "    \n",
      "    datamodule = FlattenedDataModule(**config[\"data\"])\n",
      "    model = LitMLP(**config[\"models\"])\n",
      "    \n",
      "    trainer = Trainer(gpus=8, num_nodes=4, strategy=\"ddp\")\n",
      "    trainer.fit(model, datamodule=datamodule)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    \n",
      "    config = {\n",
      "        \"data\": {\n",
      "            \"batch_size\": 256,\n",
      "            \"num_workers\": 16,\n",
      "        },\n",
      "        \"models\": {\n",
      "            \"in_channels\": 20,\n",
      "            \"hidden_channels\": 100,\n",
      "            \"out_channels\": 4,\n",
      "            \"lr\": 1e-4\n",
      "        }\n",
      "    }\n",
      "\n",
      "    main(config)"
     ]
    }
   ],
   "source": [
    "!cat trainout.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd0f48-795c-4679-811b-c69d6675377e",
   "metadata": {},
   "source": [
    "In our case, the resource allocation has been configured within the sbatch script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40d459-9370-4901-b20b-feeb9d619a58",
   "metadata": {},
   "source": [
    "echo ${SLURM_NODEID} && cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb90766-1da8-43da-b7af-27e9df388981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
